# -*- coding: utf-8 -*-
"""STEAD_Event_Based_Random_Splits_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TiA2boOx1s1tePyonDyRepez8cRAqjS7

# Print some figures and statistics
"""

# -*- coding: utf-8 -*-
"""STEAD_Event_Based_Random_Splits_Preprocessing.ipynb

This notebook processes the STEAD dataset to create event-based groupings
and prepares data files for 50 different random event-based splits.
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision torchaudio
!pip install h5py pandas tqdm matplotlib torchinfo seaborn

import time
import os
import numpy as np
import pandas as pd
import h5py
from tqdm import tqdm
import torch
from collections import defaultdict
import random
import pickle
import matplotlib.pyplot as plt
import seaborn as sns

# Start time
start_time = time.time()

# Check for GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# Define the paths to your data files
file_name = "/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/STEAD/merge.hdf5"
csv_file = "/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/STEAD/merge.csv"
output_dir = "/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/STEAD_Event_Based"

# Verify file existence
assert os.path.isfile(file_name), f"HDF5 file not found at {file_name}"
assert os.path.isfile(csv_file), f"CSV file not found at {csv_file}"
os.makedirs(output_dir, exist_ok=True)

# Helper function for processing SNR values
def string_convertor(dd):
    """Convert string representation of SNR values to a list of floats"""
    dd2 = dd.split()
    SNR = []
    for d in dd2:
        if d not in ['[', ']']:
            dL = d.split('[')
            dR = d.split(']')
            if len(dL) == 2:
                dig = dL[1]
            elif len(dR) == 2:
                dig = dR[0]
            elif len(dR) == 1 and len(dL) == 1:
                dig = d
            try:
                dig = float(dig)
            except Exception:
                dig = None
            SNR.append(dig)
    return SNR

# Load and filter the dataset
print("Loading and filtering dataset...")
df = pd.read_csv(csv_file, low_memory=False)
print(f"Initial number of records: {len(df)}")

# Display the value counts for the 'trace_category' column before filtering
print("Value counts before filtering:")
trace_category_counts_before = df["trace_category"].value_counts()
print(trace_category_counts_before)

# Plot the histogram of the earthquake magnitudes before filtering
plt.figure(figsize=(10, 6))
sns.histplot(df["source_magnitude"], bins=30, kde=True)
plt.xlabel('Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.grid(True)
max_magnitude = df["source_magnitude"].max()
min_magnitude = df["source_magnitude"].min()
plt.text(6.5, 120000, f'Max: {max_magnitude:.2f} M\nMin: {min_magnitude:.2f} M',
         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)
plt.tight_layout()
plt.show()

# Ensure the 'source_origin_time' column is in datetime format
df['source_origin_time'] = pd.to_datetime(df['source_origin_time'])

# Sort by source_origin_time to ensure chronological order
df = df.sort_values(by='source_origin_time')

# Apply the filtering criteria
print("Applying filtering criteria...")
df = df[df.trace_category == 'earthquake_local']
df = df[df.source_distance_km <= 110]
df = df[df.source_magnitude_type == 'ml']
df = df[df.p_arrival_sample >= 200]
df = df[df.p_arrival_sample + 2900 <= 6000]  # Ensure window fits within seismogram
df = df[df.p_arrival_sample <= 1500]
df = df[df.s_arrival_sample >= 200]
df = df[df.s_arrival_sample <= 2500]

# Fix coda_end_sample column parsing
df['coda_end_sample'] = df['coda_end_sample'].apply(lambda x: float(x.strip('[]')))
df = df.dropna(subset=['coda_end_sample'])
df = df[df['coda_end_sample'] <= 3000]
df = df[df.p_travel_sec.notnull()]
df = df[df.p_travel_sec > 0]
df = df[df.source_distance_km.notnull()]
df = df[df.source_distance_km > 0]
df = df[df.source_depth_km.notnull()]
df = df[df.source_magnitude.notnull()]
df = df[df.back_azimuth_deg.notnull()]
df = df[df.back_azimuth_deg > 0]
df.snr_db = df.snr_db.apply(lambda x: np.mean(string_convertor(x)))
df = df[df.snr_db >= 20]

# Print number of records after filtering
print(f"Number of records after initial filtering: {len(df)}")

# Plot the histogram of the earthquake magnitudes after filtering
plt.figure(figsize=(10, 6))
sns.histplot(df["source_magnitude"], bins=30, kde=True, color='brown')
plt.xlabel('Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.grid(True)
max_magnitude = df["source_magnitude"].max()
min_magnitude = df["source_magnitude"].min()
plt.text(4, 35000, f'Max: {max_magnitude:.2f} M\nMin: {min_magnitude:.2f} M',
         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)
plt.tight_layout()
plt.show()

# Implementing multi-observations with a threshold of >= 400
print("Filtering for multi-observations stations...")
uniq_ins = df.receiver_code.unique()
labM = []

print("Counting observations per station...")
for station in tqdm(uniq_ins):
    count = sum(df.receiver_code == station)
    if count >= 400:
        labM.append(station)
        print(f"{station}: {count} observations")

print(f"Number of stations with ≥400 observations: {len(labM)}")

# # Save the multi-observations stations list
# multi_observations_path = os.path.join(output_dir, "multi_observations.npy")
# np.save(multi_observations_path, labM)

# Filter the dataset to include only records from stations with ≥400 observations
df_filtered = df[df.receiver_code.isin(labM)]
print(f"Number of records after multi-observations filtering: {len(df_filtered)}")

# Group seismograms by their source event ID
print("Grouping seismograms by source event ID...")
event_to_seismograms = defaultdict(list)
seismogram_to_event = {}
event_metadata = {}

for index, row in df_filtered.iterrows():
    event_id = row['source_id']
    seismogram_id = row['trace_name']

    event_to_seismograms[event_id].append(seismogram_id)
    seismogram_to_event[seismogram_id] = event_id

    # Store metadata for each event (only once per event)
    if event_id not in event_metadata:
        event_metadata[event_id] = {
            'magnitude': row['source_magnitude'],
            'latitude': row['source_latitude'],
            'longitude': row['source_longitude'],
            'depth': row['source_depth_km'],
            'origin_time': row['source_origin_time']
        }

print(f"Number of unique earthquake events: {len(event_to_seismograms)}")
print(f"Average seismograms per event: {len(df_filtered) / len(event_to_seismograms):.2f}")

# Calculate the frequency of events with each number of seismograms
seismograms_per_event = [len(seismograms) for seismograms in event_to_seismograms.values()]

# Print summary statistics
print("\nDistribution statistics of seismograms per event:")
print(f"Minimum: {min(seismograms_per_event)} seismograms per event")
print(f"Maximum: {max(seismograms_per_event)} seismograms per event")

# Count frequency of each number of seismograms
counts = {}
for count in seismograms_per_event:
    counts[count] = counts.get(count, 0) + 1

# Output the counts
print("\nDetailed frequency distribution:")
for num_seismograms in range(1, max(seismograms_per_event) + 1):
    num_events = counts.get(num_seismograms, 0)
    if num_events > 0:  # Only print non-zero counts to save space
        print(f"Number of events with {num_seismograms} seismogram{'s' if num_seismograms > 1 else ''}: {num_events}")

# Analyze the distribution of seismograms per event
seismograms_per_event = [len(seismograms) for seismograms in event_to_seismograms.values()]
plt.figure(figsize=(10, 6))
plt.hist(seismograms_per_event, bins=30)
plt.xlabel('Number of Seismograms per Event', fontweight='bold', fontsize=14)
plt.ylabel('Frequency', fontweight='bold', fontsize=14)
plt.title('Distribution of Seismograms per Earthquake Event', fontweight='bold', fontsize=14)
plt.grid(True)
plt.tight_layout()
plt.show()

"""# Main Preprocessing Section"""

# -*- coding: utf-8 -*-
"""STEAD_Event_Based_Random_Splits_Preprocessing.ipynb

This notebook processes the STEAD dataset to create event-based groupings
and prepares data files for 50 different random event-based splits.
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision torchaudio
!pip install h5py pandas tqdm matplotlib torchinfo seaborn

import time
import os
import numpy as np
import pandas as pd
import h5py
from tqdm import tqdm
import torch
from collections import defaultdict
import random
import pickle
import matplotlib.pyplot as plt
import seaborn as sns

# Start time
start_time = time.time()

# Check for GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# Define the paths to your data files
file_name = "/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/STEAD/merge.hdf5"
csv_file = "/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/STEAD/merge.csv"
output_dir = "/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/STEAD_Event_Based"

# Verify file existence
assert os.path.isfile(file_name), f"HDF5 file not found at {file_name}"
assert os.path.isfile(csv_file), f"CSV file not found at {csv_file}"
os.makedirs(output_dir, exist_ok=True)

# Helper function for processing SNR values
def string_convertor(dd):
    """Convert string representation of SNR values to a list of floats"""
    dd2 = dd.split()
    SNR = []
    for d in dd2:
        if d not in ['[', ']']:
            dL = d.split('[')
            dR = d.split(']')
            if len(dL) == 2:
                dig = dL[1]
            elif len(dR) == 2:
                dig = dR[0]
            elif len(dR) == 1 and len(dL) == 1:
                dig = d
            try:
                dig = float(dig)
            except Exception:
                dig = None
            SNR.append(dig)
    return SNR

# Load and filter the dataset
print("Loading and filtering dataset...")
df = pd.read_csv(csv_file, low_memory=False)
print(f"Initial number of records: {len(df)}")

# Display the value counts for the 'trace_category' column before filtering
print("Value counts before filtering:")
trace_category_counts_before = df["trace_category"].value_counts()
print(trace_category_counts_before)

# Plot the histogram of the earthquake magnitudes before filtering
plt.figure(figsize=(10, 6))
sns.histplot(df["source_magnitude"], bins=30, kde=True)
plt.xlabel('Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.grid(True)
max_magnitude = df["source_magnitude"].max()
min_magnitude = df["source_magnitude"].min()
plt.text(6.5, 120000, f'Max: {max_magnitude:.2f} M\nMin: {min_magnitude:.2f} M',
         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)
plt.tight_layout()
plt.show()

# Ensure the 'source_origin_time' column is in datetime format
df['source_origin_time'] = pd.to_datetime(df['source_origin_time'])

# Sort by source_origin_time to ensure chronological order
df = df.sort_values(by='source_origin_time')

# Apply the filtering criteria
print("Applying filtering criteria...")
df = df[df.trace_category == 'earthquake_local']
df = df[df.source_distance_km <= 110]
df = df[df.source_magnitude_type == 'ml']
df = df[df.p_arrival_sample >= 200]
df = df[df.p_arrival_sample + 2900 <= 6000]  # Ensure window fits within seismogram
df = df[df.p_arrival_sample <= 1500]
df = df[df.s_arrival_sample >= 200]
df = df[df.s_arrival_sample <= 2500]

# Fix coda_end_sample column parsing
df['coda_end_sample'] = df['coda_end_sample'].apply(lambda x: float(x.strip('[]')))
df = df.dropna(subset=['coda_end_sample'])
df = df[df['coda_end_sample'] <= 3000]
df = df[df.p_travel_sec.notnull()]
df = df[df.p_travel_sec > 0]
df = df[df.source_distance_km.notnull()]
df = df[df.source_distance_km > 0]
df = df[df.source_depth_km.notnull()]
df = df[df.source_magnitude.notnull()]
df = df[df.back_azimuth_deg.notnull()]
df = df[df.back_azimuth_deg > 0]
df.snr_db = df.snr_db.apply(lambda x: np.mean(string_convertor(x)))
df = df[df.snr_db >= 20]

# Print number of records after filtering
print(f"Number of records after initial filtering: {len(df)}")

# Plot the histogram of the earthquake magnitudes after filtering
plt.figure(figsize=(10, 6))
sns.histplot(df["source_magnitude"], bins=30, kde=True, color='brown')
plt.xlabel('Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.grid(True)
max_magnitude = df["source_magnitude"].max()
min_magnitude = df["source_magnitude"].min()
plt.text(4, 35000, f'Max: {max_magnitude:.2f} M\nMin: {min_magnitude:.2f} M',
         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)
plt.tight_layout()
plt.show()

# Implementing multi-observations with a threshold of >= 400
print("Filtering for multi-observations stations...")
uniq_ins = df.receiver_code.unique()
labM = []

print("Counting observations per station...")
for station in tqdm(uniq_ins):
    count = sum(df.receiver_code == station)
    if count >= 400:
        labM.append(station)
        print(f"{station}: {count} observations")

print(f"Number of stations with ≥400 observations: {len(labM)}")

# # Save the multi-observations stations list
# multi_observations_path = os.path.join(output_dir, "multi_observations.npy")
# np.save(multi_observations_path, labM)

# Filter the dataset to include only records from stations with ≥400 observations
df_filtered = df[df.receiver_code.isin(labM)]
print(f"Number of records after multi-observations filtering: {len(df_filtered)}")

# Group seismograms by their source event ID
print("Grouping seismograms by source event ID...")
event_to_seismograms = defaultdict(list)
seismogram_to_event = {}
event_metadata = {}

for index, row in df_filtered.iterrows():
    event_id = row['source_id']
    seismogram_id = row['trace_name']

    event_to_seismograms[event_id].append(seismogram_id)
    seismogram_to_event[seismogram_id] = event_id

    # Store metadata for each event (only once per event)
    if event_id not in event_metadata:
        event_metadata[event_id] = {
            'magnitude': row['source_magnitude'],
            'latitude': row['source_latitude'],
            'longitude': row['source_longitude'],
            'depth': row['source_depth_km'],
            'origin_time': row['source_origin_time']
        }

print(f"Number of unique earthquake events: {len(event_to_seismograms)}")
print(f"Average seismograms per event: {len(df_filtered) / len(event_to_seismograms):.2f}")

# Analyze the distribution of seismograms per event
seismograms_per_event = [len(seismograms) for seismograms in event_to_seismograms.values()]
plt.figure(figsize=(10, 6))
plt.hist(seismograms_per_event, bins=30)
plt.xlabel('Number of Seismograms per Event', fontweight='bold', fontsize=14)
plt.ylabel('Frequency', fontweight='bold', fontsize=14)
plt.title('Distribution of Seismograms per Earthquake Event', fontweight='bold', fontsize=14)
plt.grid(True)
plt.tight_layout()
plt.show()

# Check which seismograms exist in the HDF5 file
print("Checking available seismograms in HDF5 file...")
with h5py.File(file_name, 'r') as dtfl:
    # List all available seismogram IDs in the HDF5 file
    available_seismogram_ids = set([key for key in dtfl['data'].keys()])
    print(f"Total seismograms in HDF5 file: {len(available_seismogram_ids)}")

# Find the intersection of seismogram IDs that exist in both dataframe and HDF5 file
valid_seismogram_ids = [seis_id for seis_id in df_filtered['trace_name'] if seis_id in available_seismogram_ids]
print(f"Number of valid seismograms found in HDF5: {len(valid_seismogram_ids)}")

# Rebuild the event-to-seismograms mapping with only valid seismograms
valid_event_to_seismograms = defaultdict(list)
for seis_id in valid_seismogram_ids:
    event_id = seismogram_to_event[seis_id]
    valid_event_to_seismograms[event_id].append(seis_id)

# Remove events that have no valid seismograms left
valid_event_to_seismograms = {k: v for k, v in valid_event_to_seismograms.items() if len(v) > 0}
print(f"Number of events with valid seismograms: {len(valid_event_to_seismograms)}")

# Function to load seismogram data from HDF5 file
def load_seismogram(file_name, seismogram_id):
    """
    Load a single seismogram from the HDF5 file

    Args:
        file_name: Path to the HDF5 file
        seismogram_id: ID of the seismogram to load

    Returns:
        Tuple of (data, magnitude, p_arrival) or (None, None, None) if seismogram not found

    Notes:
        - Data is a 30-second window centered on the P-arrival time
        - Specifically, the window starts 100 samples (1s at 100Hz) before P-arrival
          and extends for 3000 samples (30s at 100Hz)
    """
    with h5py.File(file_name, 'r') as dtfl:
        dataset = dtfl.get(f'data/{seismogram_id}')
        if dataset is None:
            return None, None, None

        data = np.array(dataset)
        # Extract P-arrival sample point (this is the index where P-wave arrives)
        spt = int(dataset.attrs['p_arrival_sample'])

        # Extract 30-second window: 1s before P-arrival and 29s after
        # This window size was chosen to capture both P and S arrivals
        # while maintaining a consistent size for all seismograms
        dshort = data[spt-100:spt+2900, :]

        # Get the earthquake magnitude (rounded to 2 decimal places)
        mag = round(float(dataset.attrs['source_magnitude']), 2)

        return dshort, mag, spt

# Load all valid seismograms
print("Loading all valid seismograms...")
all_data = []
all_labels = []
all_event_ids = []  # To track which event each seismogram belongs to
all_seismogram_ids = []  # To track the original seismogram ID

# Event data dict to store waveforms by event
event_waveform_data = {}

for event_id, seismogram_ids in tqdm(valid_event_to_seismograms.items(), desc="Loading events"):
    event_waveforms = []

    for seis_id in seismogram_ids:
        data, label, p_arrival = load_seismogram(file_name, seis_id)
        if data is None:
            continue

        all_data.append(data)
        all_labels.append(label)
        all_event_ids.append(event_id)
        all_seismogram_ids.append(seis_id)

        # Store for event visualization
        event_waveforms.append({
            'data': data,
            'magnitude': label,
            'p_arrival': p_arrival,
            'seismogram_id': seis_id
        })

    event_waveform_data[event_id] = event_waveforms

# Convert lists to numpy arrays
all_data = np.array(all_data)
all_labels = np.array(all_labels)
all_event_ids = np.array(all_event_ids)
all_seismogram_ids = np.array(all_seismogram_ids)

print(f"Final dataset: {len(all_data)} seismograms from {len(np.unique(all_event_ids))} events")
print(f"Data shape: {all_data.shape}, Labels shape: {all_labels.shape}")

# Save the mapping between seismogram indices and their IDs
with open(os.path.join(output_dir, 'seismogram_indices.pkl'), 'wb') as f:
    pickle.dump({
        'seismogram_ids': all_seismogram_ids,
        'event_ids': all_event_ids
    }, f)

# Visualize example events showing all seismograms from the same event
# Find events with multiple seismograms for visualization
events_with_multiple_seismograms = []
for event_id, waveforms in event_waveform_data.items():
    if len(waveforms) > 1:
        events_with_multiple_seismograms.append((event_id, len(waveforms)))

# Sort by number of seismograms (descending)
events_with_multiple_seismograms.sort(key=lambda x: x[1], reverse=True)

# Function to visualize all seismograms from an event
def visualize_event_seismograms(event_id, max_seismograms=6):
    """
    Visualize all seismograms from a specific event to show event grouping

    Args:
        event_id: ID of the event to visualize
        max_seismograms: Maximum number of seismograms to display
    """
    waveforms = event_waveform_data[event_id]
    event_info = event_metadata.get(event_id, {})

    # Limit the number of seismograms to display
    num_seismograms = min(len(waveforms), max_seismograms)
    waveforms = waveforms[:num_seismograms]

    # Create a figure with subplots for each seismogram (3 components each)
    fig = plt.figure(figsize=(15, num_seismograms * 5))
    fig.suptitle(f"Event ID: {event_id}\nMagnitude: {event_info.get('magnitude', 'N/A')}, "
                f"Depth: {event_info.get('depth', 'N/A')} km\n"
                f"Location: ({event_info.get('latitude', 'N/A')}, {event_info.get('longitude', 'N/A')})\n"
                f"Total Seismograms: {len(event_waveform_data[event_id])}",
                fontsize=16, fontweight='bold')

    components = ['East-West', 'North-South', 'Vertical']
    time = np.arange(3000) / 100  # Convert to seconds (100Hz sampling)

    for i, waveform in enumerate(waveforms):
        data = waveform['data']
        seis_id = waveform['seismogram_id']

        for j in range(3):
            ax = fig.add_subplot(num_seismograms, 3, i*3 + j + 1)
            ax.plot(time, data[:, j], 'k')

            ax.set_title(f"Seismogram {i+1} - {components[j]}")
            ax.set_xlabel('Time (seconds)')
            ax.set_ylabel('Amplitude')
            ax.axvline(x=1.0, color='blue', linestyle='--', linewidth=1, label='P-arrival')
            ax.grid(True)

            # Only add legend to the first subplot
            if i == 0 and j == 0:
                ax.legend()

            # Add seismogram ID as a text annotation
            if j == 2:  # Add to the vertical component only
                ax.annotate(f"ID: {seis_id}", xy=(0.5, -0.4), xycoords='axes fraction',
                            fontsize=10, ha='center')

    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust for the suptitle
    plt.savefig(os.path.join(output_dir, f'event_{event_id}_seismograms.png'), dpi=150)
    plt.show()

# Visualize a few events with multiple seismograms
print("\nVisualizing example events with multiple seismograms:")
for i, (event_id, count) in enumerate(events_with_multiple_seismograms[:3]):
    print(f"Example {i+1}: Event {event_id} with {count} seismograms")
    visualize_event_seismograms(event_id)

# Convert to tensors
all_data_tensor = torch.tensor(all_data, dtype=torch.float32)
all_labels_tensor = torch.tensor(all_labels, dtype=torch.float32)

# Save the entire dataset and associated event IDs
print("Saving data files...")
torch.save(all_data_tensor, os.path.join(output_dir, 'all_data.pt'))
torch.save(all_labels_tensor, os.path.join(output_dir, 'all_labels.pt'))

# Save the mapping between indices and event IDs for future reference
with open(os.path.join(output_dir, 'index_to_event_id.pkl'), 'wb') as f:
    pickle.dump({
        'all_event_ids': all_event_ids,
        'event_to_seismograms': valid_event_to_seismograms,
        'event_metadata': event_metadata
    }, f)

# Create function to generate the splits file without using seeds
# This file will be used by the second notebook which will apply its own seeds
def generate_event_based_split_indices(all_event_ids, train_ratio=0.7, val_ratio=0.1):
    """
    Generate the splits file without applying any shuffling
    The second notebook will handle the randomization with specific seeds

    Args:
        all_event_ids: Array of event IDs corresponding to each seismogram
        train_ratio: Proportion for training set
        val_ratio: Proportion for validation set

    Returns:
        Dictionary with unique events and their corresponding seismogram indices
    """
    # Get unique event IDs (maintaining order)
    unique_events = []
    for event_id in all_event_ids:
        if event_id not in unique_events:
            unique_events.append(event_id)

    unique_events = np.array(unique_events)
    print(f"Found {len(unique_events)} unique events for split preparation")

    # Get the indices for each event
    event_indices = {}
    for event_id in unique_events:
        indices = np.where(all_event_ids == event_id)[0]
        event_indices[event_id] = indices

    return {
        'unique_events': unique_events,
        'event_indices': event_indices,
        'train_ratio': train_ratio,
        'val_ratio': val_ratio
    }

# Generate the splits file (without applying randomization)
print("Preparing split information for second notebook...")
split_info = generate_event_based_split_indices(all_event_ids)

# Save the split information
with open(os.path.join(output_dir, 'event_split_info.pkl'), 'wb') as f:
    pickle.dump(split_info, f)

# Print a summary of what was created
print("\nFiles created in the output directory:")
print(f"1. all_data.pt - All seismogram data tensors")
print(f"2. all_labels.pt - All magnitude labels")
print(f"3. index_to_event_id.pkl - Mapping between indices and event IDs")
print(f"4. event_split_info.pkl - Information for creating random event-based splits")
print(f"5. multi_observations.npy - List of stations with ≥400 observations")
print(f"6. seismogram_indices.pkl - Mapping between indices and seismogram IDs")
print(f"7. Event visualization images (PNG files)")

# Print example events for reference
print("\nExample events with multiple seismograms:")
for i, (event_id, count) in enumerate(events_with_multiple_seismograms[:5]):
    print(f"Event {event_id}: {count} seismograms")

# Report execution time
end_time = time.time()
print(f"\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes")

print("\nPreprocessing complete! Now you can run the 50 experiments notebook.")