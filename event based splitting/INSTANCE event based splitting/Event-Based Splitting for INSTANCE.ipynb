{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afdbabfe",
   "metadata": {},
   "source": [
    "Cell 1: Initial Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49107d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"INSTANCE_Event_Based_Random_Splits_Preprocessing.ipynb\n",
    "\n",
    "This notebook processes the INSTANCE dataset to create event-based groupings\n",
    "and prepares data files for 50 different random event-based splits.\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install h5py pandas tqdm matplotlib torchinfo seaborn\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.signal\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define the paths to your data files\n",
    "metadata_path = \"/content/drive/My Drive/INSTANCE_Dataset/metadata.csv\"\n",
    "hdf5_path = \"/content/drive/My Drive/INSTANCE_Dataset/waveforms.hdf5\"\n",
    "output_dir = \"/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/INSTANCE_Event_Based\"\n",
    "\n",
    "# Verify file existence\n",
    "assert os.path.isfile(metadata_path), f\"Metadata file not found at {metadata_path}\"\n",
    "assert os.path.isfile(hdf5_path), f\"HDF5 file not found at {hdf5_path}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59e77e",
   "metadata": {},
   "source": [
    "Cell 2: Data Loading and Initial Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and access the metadata from the CSV file\n",
    "metadata = pd.read_csv(metadata_path, low_memory=False)\n",
    "print(f\"Initial number of metadata entries: {len(metadata)}\")\n",
    "\n",
    "# Ensure the 'source_origin_time' column is in datetime format\n",
    "metadata['source_origin_time'] = pd.to_datetime(metadata['source_origin_time'])\n",
    "\n",
    "# Sort by source_origin_time to ensure chronological order\n",
    "metadata = metadata.sort_values(by='source_origin_time')\n",
    "\n",
    "# Filter metadata for dates from September 2018 onwards\n",
    "from datetime import datetime, timezone\n",
    "start_date = datetime(2018, 9, 1, tzinfo=timezone.utc)\n",
    "metadata = metadata[metadata['source_origin_time'] >= start_date]\n",
    "\n",
    "print(f\"Number of metadata entries from 1st September 2018 to 31st January 2020: {len(metadata)}\")\n",
    "\n",
    "# Plot the histogram of the earthquake magnitudes before filtering\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(metadata[\"source_magnitude\"], bins=30, kde=True)\n",
    "plt.xlabel('Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "max_magnitude = metadata[\"source_magnitude\"].max()\n",
    "min_magnitude = metadata[\"source_magnitude\"].min()\n",
    "plt.text(0.7*max_magnitude, plt.gca().get_ylim()[1]*0.8,\n",
    "         f'Max: {max_magnitude:.2f} M\\nMin: {min_magnitude:.2f} M',\n",
    "         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of metadata entries now: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9670a43",
   "metadata": {},
   "source": [
    "Cell 3: Apply Data Filtering Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Create a deep copy of the metadata\n",
    "filtered_metadata = copy.deepcopy(metadata)\n",
    "\n",
    "# Set sampling rate\n",
    "filtered_metadata['trace_sampling_rate_hz'] = 100\n",
    "\n",
    "# Adjusted filtering for INSTANCE dataset with 30s window\n",
    "filters = [\n",
    "    ('path_ep_distance_km <= 110', filtered_metadata.path_ep_distance_km <= 110),\n",
    "    ('source_magnitude_type == ML', filtered_metadata.source_magnitude_type == 'ML'),\n",
    "    ('trace_P_arrival_sample.notnull()', filtered_metadata.trace_P_arrival_sample.notnull()),\n",
    "    ('path_travel_time_P_s.notnull()', filtered_metadata.path_travel_time_P_s.notnull()),\n",
    "    ('path_travel_time_P_s > 0', filtered_metadata.path_travel_time_P_s > 0),\n",
    "    ('path_ep_distance_km.notnull()', filtered_metadata.path_ep_distance_km.notnull()),\n",
    "    ('path_ep_distance_km > 0', filtered_metadata.path_ep_distance_km > 0),\n",
    "    ('source_depth_km.notnull()', filtered_metadata.source_depth_km.notnull()),\n",
    "    ('source_magnitude.notnull()', filtered_metadata.source_magnitude.notnull()),\n",
    "    ('path_backazimuth_deg.notnull()', filtered_metadata.path_backazimuth_deg.notnull()),\n",
    "    ('path_backazimuth_deg > 0', filtered_metadata.path_backazimuth_deg > 0),\n",
    "]\n",
    "\n",
    "# Apply filters one by one and keep track of the data\n",
    "for desc, filt in filters:\n",
    "    filtered_metadata = filtered_metadata[filt]\n",
    "    print(f\"After filter '{desc}': {len(filtered_metadata)} entries\")\n",
    "\n",
    "# Ensure at least 30 seconds of data after P-arrival\n",
    "filtered_metadata = filtered_metadata[filtered_metadata.trace_P_arrival_sample + filtered_metadata.trace_sampling_rate_hz * 30 <= filtered_metadata.trace_npts]\n",
    "print(f\"After ensuring 30s after P arrival: {len(filtered_metadata)} entries\")\n",
    "\n",
    "# Calculate time window statistics\n",
    "filtered_metadata['window_start'] = filtered_metadata.trace_P_arrival_sample - filtered_metadata.trace_sampling_rate_hz * 5  # 5 seconds before P arrival\n",
    "filtered_metadata['window_end'] = filtered_metadata.trace_P_arrival_sample + filtered_metadata.trace_sampling_rate_hz * 25  # 25 seconds after P arrival (total 30s window)\n",
    "\n",
    "# Ensure window start is not negative\n",
    "filtered_metadata = filtered_metadata[filtered_metadata.window_start >= 0]\n",
    "print(f\"After ensuring non-negative window start: {len(filtered_metadata)} entries\")\n",
    "\n",
    "print(f\"\\nFinal number of filtered metadata entries: {len(filtered_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5113b0",
   "metadata": {},
   "source": [
    "Cell 4: Apply SNR Filter and Multi-Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SNR\n",
    "# Check if 'snr_db' column exists, if not, use appropriate column names from INSTANCE dataset\n",
    "if 'snr_db' in filtered_metadata.columns:\n",
    "    def string_convertor(dd):\n",
    "        if isinstance(dd, str):\n",
    "            dd2 = dd.split()\n",
    "            SNR = []\n",
    "            for d in dd2:\n",
    "                if d not in ['[', ']']:\n",
    "                    dL = d.split('[')\n",
    "                    dR = d.split(']')\n",
    "                    if len(dL) == 2:\n",
    "                        dig = dL[1]\n",
    "                    elif len(dR) == 2:\n",
    "                        dig = dR[0]\n",
    "                    elif len(dR) == 1 and len(dL) == 1:\n",
    "                        dig = d\n",
    "                    try:\n",
    "                        dig = float(dig)\n",
    "                    except Exception:\n",
    "                        dig = None\n",
    "                    SNR.append(dig)\n",
    "            return np.mean([x for x in SNR if x is not None])\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    filtered_metadata['snr_db'] = filtered_metadata.snr_db.apply(string_convertor)\n",
    "    filtered_metadata = filtered_metadata[filtered_metadata.snr_db >= 20]\n",
    "else:\n",
    "    # Use appropriate SNR columns from INSTANCE dataset\n",
    "    snr_columns = ['trace_E_snr_db', 'trace_N_snr_db', 'trace_Z_snr_db']\n",
    "    filtered_metadata['avg_snr_db'] = filtered_metadata[snr_columns].mean(axis=1)\n",
    "    filtered_metadata = filtered_metadata[filtered_metadata.avg_snr_db >= 20]\n",
    "\n",
    "print(f\"Number of records after SNR filtering: {len(filtered_metadata)}\")\n",
    "\n",
    "# Plot the histogram of the earthquake magnitudes after filtering\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_metadata[\"source_magnitude\"], bins=30, kde=True, color='brown')\n",
    "plt.xlabel('Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "max_magnitude = filtered_metadata[\"source_magnitude\"].max()\n",
    "min_magnitude = filtered_metadata[\"source_magnitude\"].min()\n",
    "plt.text(0.7*max_magnitude, plt.gca().get_ylim()[1]*0.8,\n",
    "         f'Max: {max_magnitude:.2f} M\\nMin: {min_magnitude:.2f} M',\n",
    "         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Implementing multi-observations with a threshold of >= 400\n",
    "print(\"Filtering for multi-observations stations...\")\n",
    "uniq_ins = filtered_metadata.station_code.unique()\n",
    "labM = []\n",
    "\n",
    "print(\"Counting observations per station...\")\n",
    "for station in tqdm(uniq_ins):\n",
    "    count = sum(filtered_metadata.station_code == station)\n",
    "    if count >= 400:\n",
    "        labM.append(station)\n",
    "        print(f\"{station}: {count} observations\")\n",
    "\n",
    "print(f\"Number of stations with ≥400 observations: {len(labM)}\")\n",
    "\n",
    "# Save the multi-observations stations list\n",
    "multi_observations_path = os.path.join(output_dir, \"multi_observations.npy\")\n",
    "np.save(multi_observations_path, labM)\n",
    "\n",
    "# Filter the dataset to include only records from stations with ≥400 observations\n",
    "df_filtered = filtered_metadata[filtered_metadata.station_code.isin(labM)]\n",
    "print(f\"Number of records after multi-observations filtering: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea6747",
   "metadata": {},
   "source": [
    "Cell 5: Group Seismograms by Events (KEY ADAPTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group seismograms by their source event ID (THIS IS THE KEY EVENT-BASED LOGIC)\n",
    "print(\"Grouping seismograms by source event ID...\")\n",
    "event_to_seismograms = defaultdict(list)\n",
    "seismogram_to_event = {}\n",
    "event_metadata = {}\n",
    "\n",
    "for index, row in df_filtered.iterrows():\n",
    "    # Use source_id as the event identifier (adapt this if INSTANCE uses a different field)\n",
    "    event_id = row['source_id'] if 'source_id' in row else row['source_event_id']  # Adapt field name as needed\n",
    "    seismogram_id = row['trace_name']\n",
    "    \n",
    "    event_to_seismograms[event_id].append(seismogram_id)\n",
    "    seismogram_to_event[seismogram_id] = event_id\n",
    "    \n",
    "    # Store metadata for each event (only once per event)\n",
    "    if event_id not in event_metadata:\n",
    "        event_metadata[event_id] = {\n",
    "            'magnitude': row['source_magnitude'],\n",
    "            'latitude': row['source_latitude'],\n",
    "            'longitude': row['source_longitude'],\n",
    "            'depth': row['source_depth_km'],\n",
    "            'origin_time': row['source_origin_time']\n",
    "        }\n",
    "\n",
    "print(f\"Number of unique earthquake events: {len(event_to_seismograms)}\")\n",
    "print(f\"Average seismograms per event: {len(df_filtered) / len(event_to_seismograms):.2f}\")\n",
    "\n",
    "# Analyze the distribution of seismograms per event\n",
    "seismograms_per_event = [len(seismograms) for seismograms in event_to_seismograms.values()]\n",
    "\n",
    "# Calculate the frequency of events with each number of seismograms\n",
    "print(\"\\nDistribution statistics of seismograms per event:\")\n",
    "print(f\"Minimum: {min(seismograms_per_event)} seismograms per event\")\n",
    "print(f\"Maximum: {max(seismograms_per_event)} seismograms per event\")\n",
    "\n",
    "# Count frequency of each number of seismograms\n",
    "counts = {}\n",
    "for count in seismograms_per_event:\n",
    "    counts[count] = counts.get(count, 0) + 1\n",
    "\n",
    "# Output the counts\n",
    "print(\"\\nDetailed frequency distribution:\")\n",
    "for num_seismograms in range(1, min(21, max(seismograms_per_event) + 1)):  # Show up to 20 for readability\n",
    "    num_events = counts.get(num_seismograms, 0)\n",
    "    if num_events > 0:\n",
    "        print(f\"Number of events with {num_seismograms} seismogram{'s' if num_seismograms > 1 else ''}: {num_events}\")\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(seismograms_per_event, bins=30)\n",
    "plt.xlabel('Number of Seismograms per Event', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Frequency', fontweight='bold', fontsize=14)\n",
    "plt.title('Distribution of Seismograms per Earthquake Event', fontweight='bold', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96726f15",
   "metadata": {},
   "source": [
    "Cell 6: Check Available Seismograms and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38467ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which seismograms exist in the HDF5 file\n",
    "print(\"Checking available seismograms in HDF5 file...\")\n",
    "\n",
    "def explore_hdf5_structure(file_path):\n",
    "    \"\"\"Explore the HDF5 structure to understand data organization\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        print(\"HDF5 structure:\")\n",
    "        f.visititems(lambda name, obj: print(f\"  {name}: {type(obj)}\"))\n",
    "\n",
    "explore_hdf5_structure(hdf5_path)\n",
    "\n",
    "# Get list of valid seismograms from the filtered metadata\n",
    "valid_seismogram_ids = df_filtered['trace_name'].tolist()\n",
    "print(f\"Number of seismograms to load: {len(valid_seismogram_ids)}\")\n",
    "\n",
    "# Rebuild the event-to-seismograms mapping with only valid seismograms\n",
    "valid_event_to_seismograms = defaultdict(list)\n",
    "for seis_id in valid_seismogram_ids:\n",
    "    event_id = seismogram_to_event[seis_id]\n",
    "    valid_event_to_seismograms[event_id].append(seis_id)\n",
    "\n",
    "# Remove events that have no valid seismograms left\n",
    "valid_event_to_seismograms = {k: v for k, v in valid_event_to_seismograms.items() if len(v) > 0}\n",
    "print(f\"Number of events with valid seismograms: {len(valid_event_to_seismograms)}\")\n",
    "\n",
    "# Function to load seismogram data from HDF5 file (adapted for INSTANCE)\n",
    "def load_seismogram_instance(hdf5_path, seismogram_id, metadata_row):\n",
    "    \"\"\"\n",
    "    Load a single seismogram from the INSTANCE HDF5 file\n",
    "    \n",
    "    Args:\n",
    "        hdf5_path: Path to the HDF5 file\n",
    "        seismogram_id: ID of the seismogram to load (trace_name)\n",
    "        metadata_row: Row from metadata containing trace information\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (data, magnitude) or (None, None) if seismogram not found\n",
    "        \n",
    "    Notes:\n",
    "        - Data is a 30-second window: 5s before P-arrival and 25s after\n",
    "        - Resampled to 100Hz if necessary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(hdf5_path, 'r') as hdf:\n",
    "            # Parse the trace name to get bucket and trace index\n",
    "            bucket, trace_info = seismogram_id.split('$')\n",
    "            trace_index = int(trace_info.split(',')[0])\n",
    "            \n",
    "            # Retrieve waveform data\n",
    "            waveform = np.array(hdf['data'][bucket][trace_index])\n",
    "            \n",
    "            sampling_rate = metadata_row['trace_sampling_rate_hz']\n",
    "            spt = int(metadata_row['trace_P_arrival_sample'])\n",
    "            \n",
    "            # Adjust window size based on sampling rate\n",
    "            window_samples = int(30 * sampling_rate)  # 30 seconds window\n",
    "            start = max(0, spt - int(5 * sampling_rate))  # 5 seconds before P arrival\n",
    "            end = start + window_samples\n",
    "            \n",
    "            if start >= waveform.shape[1] or end > waveform.shape[1]:\n",
    "                print(f\"Skipping event {seismogram_id}: Invalid window\")\n",
    "                return None, None\n",
    "            \n",
    "            dshort = waveform[:, start:end]\n",
    "            \n",
    "            # Ensure the shape is correct\n",
    "            if dshort.shape[1] != window_samples:\n",
    "                print(f\"Skipping event {seismogram_id}: Incorrect window size\")\n",
    "                return None, None\n",
    "            \n",
    "            # Resample to 100 Hz if necessary\n",
    "            if sampling_rate != 100:\n",
    "                dshort = scipy.signal.resample(dshort, 3000, axis=1)\n",
    "            \n",
    "            mag = round(float(metadata_row['source_magnitude']), 2)\n",
    "            \n",
    "            return dshort, mag\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event {seismogram_id}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd225a",
   "metadata": {},
   "source": [
    "Cell 7: Load All Valid Seismograms and Create Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55244d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all valid seismograms\n",
    "print(\"Loading all valid seismograms...\")\n",
    "all_data = []\n",
    "all_labels = []\n",
    "all_event_ids = []  # To track which event each seismogram belongs to\n",
    "all_seismogram_ids = []  # To track the original seismogram ID\n",
    "\n",
    "# Event data dict to store waveforms by event\n",
    "event_waveform_data = {}\n",
    "\n",
    "# Create a lookup dictionary for metadata rows\n",
    "metadata_lookup = {row['trace_name']: row for _, row in df_filtered.iterrows()}\n",
    "\n",
    "for event_id, seismogram_ids in tqdm(valid_event_to_seismograms.items(), desc=\"Loading events\"):\n",
    "    event_waveforms = []\n",
    "    \n",
    "    for seis_id in seismogram_ids:\n",
    "        if seis_id not in metadata_lookup:\n",
    "            continue\n",
    "            \n",
    "        metadata_row = metadata_lookup[seis_id]\n",
    "        data, label = load_seismogram_instance(hdf5_path, seis_id, metadata_row)\n",
    "        \n",
    "        if data is None:\n",
    "            continue\n",
    "        \n",
    "        all_data.append(data)\n",
    "        all_labels.append(label)\n",
    "        all_event_ids.append(event_id)\n",
    "        all_seismogram_ids.append(seis_id)\n",
    "        \n",
    "        # Store for event visualization\n",
    "        event_waveforms.append({\n",
    "            'data': data,\n",
    "            'magnitude': label,\n",
    "            'seismogram_id': seis_id\n",
    "        })\n",
    "    \n",
    "    if event_waveforms:  # Only store events that have valid waveforms\n",
    "        event_waveform_data[event_id] = event_waveforms\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_data = np.array(all_data)\n",
    "all_labels = np.array(all_labels)\n",
    "all_event_ids = np.array(all_event_ids)\n",
    "all_seismogram_ids = np.array(all_seismogram_ids)\n",
    "\n",
    "print(f\"Final dataset: {len(all_data)} seismograms from {len(np.unique(all_event_ids))} events\")\n",
    "print(f\"Data shape: {all_data.shape}, Labels shape: {all_labels.shape}\")\n",
    "\n",
    "# Save the mapping between seismogram indices and their IDs\n",
    "with open(os.path.join(output_dir, 'seismogram_indices.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'seismogram_ids': all_seismogram_ids,\n",
    "        'event_ids': all_event_ids\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6f515",
   "metadata": {},
   "source": [
    "Cell 8: Visualize Example Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f94484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find events with multiple seismograms for visualization\n",
    "events_with_multiple_seismograms = []\n",
    "for event_id, waveforms in event_waveform_data.items():\n",
    "    if len(waveforms) > 1:\n",
    "        events_with_multiple_seismograms.append((event_id, len(waveforms)))\n",
    "\n",
    "# Sort by number of seismograms (descending)\n",
    "events_with_multiple_seismograms.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Function to visualize all seismograms from an event\n",
    "def visualize_event_seismograms_instance(event_id, max_seismograms=6):\n",
    "    \"\"\"\n",
    "    Visualize all seismograms from a specific event to show event grouping\n",
    "    \n",
    "    Args:\n",
    "        event_id: ID of the event to visualize\n",
    "        max_seismograms: Maximum number of seismograms to display\n",
    "    \"\"\"\n",
    "    waveforms = event_waveform_data[event_id]\n",
    "    event_info = event_metadata.get(event_id, {})\n",
    "    \n",
    "    # Limit the number of seismograms to display\n",
    "    num_seismograms = min(len(waveforms), max_seismograms)\n",
    "    waveforms = waveforms[:num_seismograms]\n",
    "    \n",
    "    # Create a figure with subplots for each seismogram (3 components each)\n",
    "    fig = plt.figure(figsize=(15, num_seismograms * 5))\n",
    "    fig.suptitle(f\"Event ID: {event_id}\\nMagnitude: {event_info.get('magnitude', 'N/A')}, \"\n",
    "                f\"Depth: {event_info.get('depth', 'N/A')} km\\n\"\n",
    "                f\"Location: ({event_info.get('latitude', 'N/A')}, {event_info.get('longitude', 'N/A')})\\n\"\n",
    "                f\"Total Seismograms: {len(event_waveform_data[event_id])}\",\n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    components = ['East-West', 'North-South', 'Vertical']\n",
    "    time = np.arange(3000) / 100  # Convert to seconds (100Hz sampling)\n",
    "    \n",
    "    for i, waveform in enumerate(waveforms):\n",
    "        data = waveform['data']\n",
    "        seis_id = waveform['seismogram_id']\n",
    "        \n",
    "        for j in range(3):\n",
    "            ax = fig.add_subplot(num_seismograms, 3, i*3 + j + 1)\n",
    "            ax.plot(time, data[j, :], 'k')\n",
    "            \n",
    "            ax.set_title(f\"Seismogram {i+1} - {components[j]}\")\n",
    "            ax.set_xlabel('Time (seconds)')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "            ax.axvline(x=5.0, color='blue', linestyle='--', linewidth=1, label='P-arrival')\n",
    "            ax.grid(True)\n",
    "            \n",
    "            # Only add legend to the first subplot\n",
    "            if i == 0 and j == 0:\n",
    "                ax.legend()\n",
    "            \n",
    "            # Add seismogram ID as a text annotation\n",
    "            if j == 2:  # Add to the vertical component only\n",
    "                ax.annotate(f\"ID: {seis_id}\", xy=(0.5, -0.4), xycoords='axes fraction', \n",
    "                            fontsize=10, ha='center')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust for the suptitle\n",
    "    plt.savefig(os.path.join(output_dir, f'event_{event_id}_seismograms.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few events with multiple seismograms\n",
    "print(\"\\nVisualizing example events with multiple seismograms:\")\n",
    "for i, (event_id, count) in enumerate(events_with_multiple_seismograms[:3]):\n",
    "    print(f\"Example {i+1}: Event {event_id} with {count} seismograms\")\n",
    "    visualize_event_seismograms_instance(event_id)\n",
    "\n",
    "# Plot an example single seismogram\n",
    "def plot_example_seismogram_instance(data, index=0):\n",
    "    \"\"\"Plot an example three-component seismogram\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "    time = np.arange(data.shape[2]) / 100  # Convert to seconds (100Hz sampling)\n",
    "    components = ['East-West', 'North-South', 'Vertical']\n",
    "    \n",
    "    for i in range(3):\n",
    "        axes[i].plot(time, data[index, i, :], 'k')\n",
    "        axes[i].set_ylabel(f'{components[i]}\\nAmplitude', fontweight='bold', fontsize=14)\n",
    "        axes[i].axvline(x=5.0, color='blue', linestyle='--', linewidth=2, \n",
    "                      label='P-arrival (5s mark)')\n",
    "        axes[i].grid(True)\n",
    "        axes[i].legend(fontsize=12)\n",
    "    \n",
    "    axes[2].set_xlabel('Time (seconds)', fontweight='bold', fontsize=14)\n",
    "    plt.suptitle(f'Example INSTANCE Seismogram (Magnitude: {all_labels[index]})', \n",
    "                fontweight='bold', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot an example seismogram\n",
    "if len(all_data) > 0:\n",
    "    plot_example_seismogram_instance(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e657c71",
   "metadata": {},
   "source": [
    "Cell 9: Save Data and Create Split Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "all_data_tensor = torch.tensor(all_data, dtype=torch.float32)\n",
    "all_labels_tensor = torch.tensor(all_labels, dtype=torch.float32)\n",
    "\n",
    "# Save the entire dataset and associated event IDs\n",
    "print(\"Saving data files...\")\n",
    "torch.save(all_data_tensor, os.path.join(output_dir, 'all_data.pt'))\n",
    "torch.save(all_labels_tensor, os.path.join(output_dir, 'all_labels.pt'))\n",
    "\n",
    "# Save the mapping between indices and event IDs for future reference\n",
    "with open(os.path.join(output_dir, 'index_to_event_id.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'all_event_ids': all_event_ids,\n",
    "        'event_to_seismograms': valid_event_to_seismograms,\n",
    "        'event_metadata': event_metadata\n",
    "    }, f)\n",
    "\n",
    "# Create function to generate the splits file without using seeds\n",
    "# This file will be used by the second notebook which will apply its own seeds\n",
    "def generate_event_based_split_indices(all_event_ids, train_ratio=0.7, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Generate the splits file without applying any shuffling\n",
    "    The second notebook will handle the randomization with specific seeds\n",
    "    \n",
    "    Args:\n",
    "        all_event_ids: Array of event IDs corresponding to each seismogram\n",
    "        train_ratio: Proportion for training set\n",
    "        val_ratio: Proportion for validation set\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with unique events and their corresponding seismogram indices\n",
    "    \"\"\"\n",
    "    # Get unique event IDs (maintaining order)\n",
    "    unique_events = []\n",
    "    for event_id in all_event_ids:\n",
    "        if event_id not in unique_events:\n",
    "            unique_events.append(event_id)\n",
    "    \n",
    "    unique_events = np.array(unique_events)\n",
    "    print(f\"Found {len(unique_events)} unique events for split preparation\")\n",
    "    \n",
    "    # Get the indices for each event\n",
    "    event_indices = {}\n",
    "    for event_id in unique_events:\n",
    "        indices = np.where(all_event_ids == event_id)[0]\n",
    "        event_indices[event_id] = indices\n",
    "    \n",
    "    return {\n",
    "        'unique_events': unique_events,\n",
    "        'event_indices': event_indices,\n",
    "        'train_ratio': train_ratio,\n",
    "        'val_ratio': val_ratio\n",
    "    }\n",
    "\n",
    "# Generate the splits file (without applying randomization)\n",
    "print(\"Preparing split information for second notebook...\")\n",
    "split_info = generate_event_based_split_indices(all_event_ids)\n",
    "\n",
    "# Save the split information\n",
    "with open(os.path.join(output_dir, 'event_split_info.pkl'), 'wb') as f:\n",
    "    pickle.dump(split_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e847a",
   "metadata": {},
   "source": [
    "Cell 10: Summary and Final Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77db38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of what was created\n",
    "print(\"\\nFiles created in the output directory:\")\n",
    "print(f\"1. all_data.pt - All seismogram data tensors\")\n",
    "print(f\"2. all_labels.pt - All magnitude labels\")\n",
    "print(f\"3. index_to_event_id.pkl - Mapping between indices and event IDs\")\n",
    "print(f\"4. event_split_info.pkl - Information for creating random event-based splits\")\n",
    "print(f\"5. multi_observations.npy - List of stations with ≥400 observations\")\n",
    "print(f\"6. seismogram_indices.pkl - Mapping between indices and seismogram IDs\")\n",
    "print(f\"7. Event visualization images (PNG files)\")\n",
    "\n",
    "# Print statistics about the event distribution\n",
    "seismograms_per_event = [len(event_waveform_data[event_id]) for event_id in event_waveform_data.keys()]\n",
    "print(f\"\\nEvent Distribution Statistics:\")\n",
    "print(f\"Total events: {len(event_waveform_data)}\")\n",
    "print(f\"Total seismograms: {len(all_data)}\")\n",
    "print(f\"Average seismograms per event: {np.mean(seismograms_per_event):.2f}\")\n",
    "print(f\"Median seismograms per event: {np.median(seismograms_per_event):.0f}\")\n",
    "print(f\"Max seismograms per event: {max(seismograms_per_event)}\")\n",
    "print(f\"Min seismograms per event: {min(seismograms_per_event)}\")\n",
    "\n",
    "# Print example events for reference\n",
    "print(\"\\nExample events with multiple seismograms:\")\n",
    "for i, (event_id, count) in enumerate(events_with_multiple_seismograms[:10]):\n",
    "    print(f\"Event {event_id}: {count} seismograms\")\n",
    "\n",
    "# Report execution time\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "print(\"\\nPreprocessing complete! Now you can run the 50 experiments notebook using the same logic as STEAD.\")\n",
    "print(\"\\nKey differences from STEAD:\")\n",
    "print(f\"- INSTANCE average seismograms per event: {np.mean(seismograms_per_event):.2f}\")\n",
    "print(f\"- STEAD average seismograms per event: 2.14\")\n",
    "print(f\"- INSTANCE may show more pronounced differences between event-based and seismogram-based splitting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
