{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcfe9a5",
   "metadata": {},
   "source": [
    "Cell 1: Initial Setup and Imports (26-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"INSTANCE_50_Experiments_Event_Based_Splits_Runs_26_to_50.ipynb\n",
    "\n",
    "This notebook runs experiments 26 to 50 with different event-based random splits\n",
    "of the INSTANCE dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "import pickle\n",
    "\n",
    "# Helper function to convert numpy types to Python types for JSON serialization\n",
    "def numpy_to_python(obj):\n",
    "    \"\"\"Convert numpy types to Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: numpy_to_python(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "        return [numpy_to_python(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Define the range of split seeds for this notebook\n",
    "START_SEED = 26\n",
    "END_SEED = 50\n",
    "\n",
    "# Define the offset for random seeds to avoid overlap with seismogram-based split seeds\n",
    "RANDOM_SEED_OFFSET = 50  # This will map split_seed 26→76, 27→77, etc.\n",
    "\n",
    "# Mount Google Drive if using Colab\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configure environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Define paths to data files (UPDATED FOR INSTANCE)\n",
    "base_dir = \"/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/INSTANCE_Event_Based\"\n",
    "all_data_file = os.path.join(base_dir, \"all_data.pt\")\n",
    "all_labels_file = os.path.join(base_dir, \"all_labels.pt\")\n",
    "split_info_file = os.path.join(base_dir, \"event_split_info.pkl\")\n",
    "output_dir = os.path.join(base_dir, \"experiment_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check if files exist\n",
    "assert os.path.isfile(all_data_file), f\"Data file not found at {all_data_file}\"\n",
    "assert os.path.isfile(all_labels_file), f\"Labels file not found at {all_labels_file}\"\n",
    "assert os.path.isfile(split_info_file), f\"Split info file not found at {split_info_file}\"\n",
    "\n",
    "print(\"✓ INSTANCE event-based data files found\")\n",
    "print(f\"✓ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673afca",
   "metadata": {},
   "source": [
    "Cell 2: Dataset and Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Dataset and Model Classes\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    \"\"\"Dataset class for earthquake data.\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class EarthquakeModel(nn.Module):\n",
    "    \"\"\"MagNet architecture for earthquake magnitude estimation.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EarthquakeModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool1d(4, padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(32, 100, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(200, 2)  # Output: [magnitude_prediction, log_variance]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch, time_steps, channels]\n",
    "        # Convert to [batch, channels, time_steps] for conv layers\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # First conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Second conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Prepare for LSTM: [batch, time_steps, features]\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Get the last output of the LSTM\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Output layer with magnitude prediction and uncertainty\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8839c",
   "metadata": {},
   "source": [
    "Cell 3: Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Training Components\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, run_id=None,\n",
    "                 split_num=None, model_seed=None):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.run_id = run_id\n",
    "        self.split_num = split_num\n",
    "        self.model_seed = model_seed\n",
    "        self.best_model_path = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "        self.best_model_path = os.path.join(\n",
    "            output_dir, f'best_model_Run_{self.run_id}_split_{self.split_num}_seed_{self.model_seed}.pth'\n",
    "        )\n",
    "        torch.save(model.state_dict(), self.best_model_path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Custom loss function combining prediction error and uncertainty.\n",
    "\n",
    "    This implements a negative log-likelihood loss with learned aleatoric uncertainty:\n",
    "    L = 0.5 * exp(-s) * (y_true - y_hat)^2 + 0.5 * s\n",
    "\n",
    "    where:\n",
    "    - y_hat is the predicted magnitude\n",
    "    - s is the log variance (uncertainty)\n",
    "    - y_true is the true magnitude\n",
    "\n",
    "    This loss encourages the model to predict accurate magnitudes while\n",
    "    also learning to estimate its own uncertainty.\n",
    "    \"\"\"\n",
    "    y_hat = y_pred[:, 0]    # Predicted magnitude\n",
    "    s = y_pred[:, 1]        # Predicted log variance (uncertainty)\n",
    "\n",
    "    # Compute loss: 0.5 * exp(-s) * (y_true - y_hat)^2 + 0.5 * s\n",
    "    loss = 0.5 * torch.exp(-s) * (y_true - y_hat)**2 + 0.5 * s\n",
    "\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba6d37",
   "metadata": {},
   "source": [
    "Cell 4: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd60cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Training and Evaluation Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=300, patience=5,\n",
    "                run_id=None, split_num=None, model_seed=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and learning rate scheduling.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        num_epochs: Maximum number of training epochs\n",
    "        patience: Patience for early stopping\n",
    "        run_id: Identifier for the experimental run\n",
    "        split_num: Which data split is being used (0-49)\n",
    "        model_seed: Random seed used for model initialization\n",
    "        verbose: Whether to print detailed progress\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with training history and best model path\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=np.sqrt(0.1),\n",
    "        cooldown=0, patience=4, verbose=verbose, min_lr=0.5e-6\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, verbose=verbose,\n",
    "        run_id=run_id, split_num=split_num, model_seed=model_seed\n",
    "    )\n",
    "\n",
    "    criterion = custom_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        val_loss /= len(val_loader)\n",
    "        running_loss /= len(train_loader)\n",
    "\n",
    "        # Learning rate scheduling and early stopping\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}, Loss: {running_loss:.4f}, '\n",
    "                  f'Validation Loss: {val_loss:.4f}, '\n",
    "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        train_losses.append(running_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            if verbose:\n",
    "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_model_path': early_stopping.best_model_path\n",
    "    }\n",
    "\n",
    "def estimate_uncertainty(model, data_loader, num_samples=50):\n",
    "    \"\"\"\n",
    "    Estimate model uncertainty using Monte Carlo dropout.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader for test data\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (predictions, epistemic_uncertainty, aleatoric_uncertainty, combined_uncertainty)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Enable dropout during inference for Monte Carlo sampling\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()\n",
    "\n",
    "    predictions = []\n",
    "    log_variances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            batch_predictions = []\n",
    "            batch_log_variances = []\n",
    "            for data, _ in data_loader:\n",
    "                data = data.to(device)\n",
    "                output = model(data)\n",
    "                batch_predictions.append(output[:, 0].cpu().numpy())\n",
    "                batch_log_variances.append(output[:, 1].cpu().numpy())\n",
    "            predictions.append(np.concatenate(batch_predictions))\n",
    "            log_variances.append(np.concatenate(batch_log_variances))\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    log_variances = np.array(log_variances)\n",
    "\n",
    "    # Calculate mean prediction\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "\n",
    "    # Calculate mean of squared predictions\n",
    "    yhat_squared_mean = np.mean(np.square(predictions), axis=0)\n",
    "\n",
    "    # Calculate aleatoric uncertainty from log variances\n",
    "    aleatoric_uncertainty = np.mean(np.exp(log_variances), axis=0)\n",
    "\n",
    "    # Calculate epistemic uncertainty as standard deviation of predictions\n",
    "    epistemic_uncertainty = np.std(predictions, axis=0)\n",
    "\n",
    "    # Calculate combined uncertainty\n",
    "    combined_uncertainty = yhat_squared_mean - np.square(mean_prediction) + aleatoric_uncertainty\n",
    "\n",
    "    return mean_prediction, epistemic_uncertainty, aleatoric_uncertainty, combined_uncertainty\n",
    "\n",
    "def evaluate_model(model_path, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on test data.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the saved model weights\n",
    "        test_loader: DataLoader for test data\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    model = EarthquakeModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Get predictions and uncertainties\n",
    "    mean_pred, epistemic_unc, aleatoric_unc, combined_unc = estimate_uncertainty(model, test_loader)\n",
    "\n",
    "    # Get true values\n",
    "    true_values = []\n",
    "    for _, target in test_loader:\n",
    "        true_values.append(target.numpy())\n",
    "    true_values = np.concatenate(true_values)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(mean_pred - true_values))\n",
    "\n",
    "    return {\n",
    "        'mae': float(mae),\n",
    "        'mean_prediction': mean_pred,\n",
    "        'true_values': true_values,\n",
    "        'epistemic_uncertainty': epistemic_unc,\n",
    "        'aleatoric_uncertainty': aleatoric_unc,\n",
    "        'combined_uncertainty': combined_unc,\n",
    "        'mean_epistemic_uncertainty': float(np.mean(epistemic_unc)),\n",
    "        'mean_aleatoric_uncertainty': float(np.mean(aleatoric_unc)),\n",
    "        'mean_combined_uncertainty': float(np.mean(combined_unc))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e401a2",
   "metadata": {},
   "source": [
    "Cell 5: Experimental Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Experimental Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def create_event_based_split(split_seed):\n",
    "    \"\"\"\n",
    "    Create a random event-based split with the specified seed\n",
    "\n",
    "    Args:\n",
    "        split_seed: Random seed for the split\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with train, val, test data and labels\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    all_data = torch.load(all_data_file)\n",
    "    all_labels = torch.load(all_labels_file)\n",
    "\n",
    "    # Load the split information\n",
    "    with open(split_info_file, 'rb') as f:\n",
    "        split_info = pickle.load(f)\n",
    "\n",
    "    unique_events = split_info['unique_events']\n",
    "    event_indices = split_info['event_indices']\n",
    "    train_ratio = split_info['train_ratio']\n",
    "    val_ratio = split_info['val_ratio']\n",
    "\n",
    "    # Apply the offset to get a different random seed (51-75 instead of 1-25)\n",
    "    random_seed = split_seed + RANDOM_SEED_OFFSET\n",
    "\n",
    "    # Set the seed for reproducibility using the new random seed\n",
    "    print(f\"  Using random seed {random_seed} for split {split_seed}\")\n",
    "    set_seed(random_seed)\n",
    "\n",
    "    # Create a shuffled copy of the unique events\n",
    "    events_copy = unique_events.copy()\n",
    "    random.shuffle(events_copy)\n",
    "\n",
    "    # Split events into train/val/test\n",
    "    train_size = int(train_ratio * len(events_copy))\n",
    "    val_size = int(val_ratio * len(events_copy))\n",
    "\n",
    "    train_events = events_copy[:train_size]\n",
    "    val_events = events_copy[train_size:train_size + val_size]\n",
    "    test_events = events_copy[train_size + val_size:]\n",
    "\n",
    "    # Collect indices for each split\n",
    "    train_indices = np.concatenate([event_indices[event_id] for event_id in train_events])\n",
    "    val_indices = np.concatenate([event_indices[event_id] for event_id in val_events])\n",
    "    test_indices = np.concatenate([event_indices[event_id] for event_id in test_events])\n",
    "\n",
    "    # Extract data using the indices\n",
    "    train_data = all_data[train_indices]\n",
    "    train_labels = all_labels[train_indices]\n",
    "\n",
    "    val_data = all_data[val_indices]\n",
    "    val_labels = all_labels[val_indices]\n",
    "\n",
    "    test_data = all_data[test_indices]\n",
    "    test_labels = all_labels[test_indices]\n",
    "\n",
    "    return {\n",
    "        'train_data': train_data,\n",
    "        'train_labels': train_labels,\n",
    "        'val_data': val_data,\n",
    "        'val_labels': val_labels,\n",
    "        'test_data': test_data,\n",
    "        'test_labels': test_labels,\n",
    "        'split_seed': split_seed,  # Keep the original split_seed for labeling (1-25)\n",
    "        'random_seed': random_seed,  # Save the actual random seed used (51-75)\n",
    "        'train_events': train_events,\n",
    "        'val_events': val_events,\n",
    "        'test_events': test_events\n",
    "    }\n",
    "\n",
    "def run_experiment(split_seed, model_seeds, run_id):\n",
    "    \"\"\"\n",
    "    Run a complete experiment with multiple model initializations on a specific data split.\n",
    "\n",
    "    Args:\n",
    "        split_seed: Random seed for the split\n",
    "        model_seeds: List of random seeds for model initialization\n",
    "        run_id: Identifier for this experiment run\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with experiment results\n",
    "    \"\"\"\n",
    "    print(f\"Running experiment with split seed {split_seed}\")\n",
    "\n",
    "    # Create the data split\n",
    "    split_data = create_event_based_split(split_seed)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = EarthquakeDataset(split_data['train_data'], split_data['train_labels'])\n",
    "    val_dataset = EarthquakeDataset(split_data['val_data'], split_data['val_labels'])\n",
    "    test_dataset = EarthquakeDataset(split_data['test_data'], split_data['test_labels'])\n",
    "\n",
    "    # Create dataloaders (UPDATED BATCH SIZE FOR LARGER INSTANCE DATASET)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Log split sizes\n",
    "    print(f\"  Train: {len(train_dataset)} samples from {len(split_data['train_events'])} events\")\n",
    "    print(f\"  Validation: {len(val_dataset)} samples from {len(split_data['val_events'])} events\")\n",
    "    print(f\"  Test: {len(test_dataset)} samples from {len(split_data['test_events'])} events\")\n",
    "\n",
    "    # Run experiments with multiple random initializations\n",
    "    seed_results = []\n",
    "\n",
    "    for model_seed in model_seeds:\n",
    "        print(f\"  Training with model seed {model_seed}\")\n",
    "\n",
    "        # Set random seed for model initialization\n",
    "        set_seed(model_seed)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = EarthquakeModel().to(device)\n",
    "\n",
    "        # Train the model\n",
    "        training_result = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            run_id=run_id, split_num=split_seed, model_seed=model_seed,\n",
    "            verbose=False  # Set to True for detailed progress\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        best_model_path = training_result['best_model_path']\n",
    "        evaluation_result = evaluate_model(best_model_path, test_loader)\n",
    "\n",
    "        # Store results\n",
    "        seed_results.append({\n",
    "            'model_seed': model_seed,\n",
    "            'training_history': {\n",
    "                'train_losses': training_result['train_losses'],\n",
    "                'val_losses': training_result['val_losses']\n",
    "            },\n",
    "            'evaluation': evaluation_result\n",
    "        })\n",
    "\n",
    "        print(f\"  Seed {model_seed} - MAE: {evaluation_result['mae']:.4f}\")\n",
    "\n",
    "    # Find median performance\n",
    "    sorted_results = sorted(seed_results, key=lambda x: x['evaluation']['mae'])\n",
    "    median_result = sorted_results[len(model_seeds) // 2]\n",
    "\n",
    "    return {\n",
    "        'split_seed': split_seed,\n",
    "        'random_seed_used': split_seed + RANDOM_SEED_OFFSET,  # Save the actual random seed used\n",
    "        'all_seed_results': seed_results,\n",
    "        'median_mae': median_result['evaluation']['mae'],\n",
    "        'median_model_seed': median_result['model_seed'],\n",
    "        'median_aleatoric_uncertainty': median_result['evaluation']['mean_aleatoric_uncertainty'],\n",
    "        'median_epistemic_uncertainty': median_result['evaluation']['mean_epistemic_uncertainty'],\n",
    "        'median_combined_uncertainty': median_result['evaluation']['mean_combined_uncertainty'],\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': len(val_dataset),\n",
    "        'test_size': len(test_dataset),\n",
    "        'train_events': len(split_data['train_events']),\n",
    "        'val_events': len(split_data['val_events']),\n",
    "        'test_events': len(split_data['test_events'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d49a9",
   "metadata": {},
   "source": [
    "Cell 6: Main Execution (26-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703197ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Main Execution\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define model initialization seeds (these stay fixed across all experiments)\n",
    "    model_seeds = [42, 123, 256, 789, 1024]  # 5 different model initializations\n",
    "\n",
    "    # Define the specific split seeds for this notebook\n",
    "    split_seeds = list(range(START_SEED, END_SEED + 1))\n",
    "\n",
    "    # Define results file for this range of experiments\n",
    "    results_file = os.path.join(output_dir, f\"results_{START_SEED}_to_{END_SEED}.json\")\n",
    "\n",
    "    # Run experiments with the specified split seeds\n",
    "    all_results = []\n",
    "\n",
    "    print(f\"Starting INSTANCE Event-Based Splitting Experiments {START_SEED}-{END_SEED}\")\n",
    "    print(f\"Expected: INSTANCE (avg 10.64 seismograms/event) should show MORE pronounced\")\n",
    "    print(f\"differences than STEAD (avg 2.14 seismograms/event) between splitting methods\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, split_seed in enumerate(tqdm(split_seeds, desc=f\"Running experiments {START_SEED}-{END_SEED}\")):\n",
    "        # Calculate the global run ID (to maintain consistent naming with the original code)\n",
    "        global_run_id = split_seed  # This keeps the same run_id as in the original code\n",
    "\n",
    "        # Run experiment for this split\n",
    "        result = run_experiment(split_seed, model_seeds, global_run_id)\n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results after each split\n",
    "        with open(results_file, 'w') as f:\n",
    "            # Convert numpy arrays to Python lists before serialization\n",
    "            serializable_results = numpy_to_python(all_results)\n",
    "            json.dump(serializable_results, f, indent=4)\n",
    "\n",
    "        print(f\"Completed experiment for split seed {split_seed} (using random seed {split_seed + RANDOM_SEED_OFFSET})\")\n",
    "        print(f\"Median MAE: {result['median_mae']:.4f}\")\n",
    "        print(f\"Median Aleatoric Uncertainty: {result['median_aleatoric_uncertainty']:.4f}\")\n",
    "        print(f\"Median Epistemic Uncertainty: {result['median_epistemic_uncertainty']:.4f}\")\n",
    "        print(f\"Median Combined Uncertainty: {result['median_combined_uncertainty']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\nTotal execution time: {elapsed_time/60:.2f} minutes\")\n",
    "\n",
    "    print(f\"\\nINSTANCE Experiment batch {START_SEED}-{END_SEED} completed. Results saved in:\")\n",
    "    print(f\"- {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
