{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Title: Data Preprocessing for Earthquake Magnitude Estimation\n",
    "\n",
    "This notebook implements data preprocessing for the STEAD (STanford EArthquake Dataset),\n",
    "focusing on preparing data for single-station earthquake magnitude estimation.\n",
    "\n",
    "Key Steps:\n",
    "1. Data loading and initial filtering\n",
    "2. Signal quality control (SNR thresholding)\n",
    "3. Extraction of maximum amplitude features\n",
    "4. Statistical analysis and visualizations\n",
    "\n",
    "Dependencies:\n",
    "- torch, h5py, pandas, numpy\n",
    "- matplotlib, seaborn\n",
    "- tqdm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 1: Setup and Imports\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Configure environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Loading and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 2: Data Loading and Helper Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for earthquake data.\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def load_data(file_name, file_list):\n",
    "    \"\"\"\n",
    "    Load earthquake data from HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        file_name: Path to HDF5 file\n",
    "        file_list: List of event IDs to load\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (data tensor, labels tensor)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    dtfl = h5py.File(file_name, 'r')\n",
    "    for evi in tqdm(file_list):\n",
    "        dataset = dtfl.get(f'data/{evi}')\n",
    "        if dataset is None:\n",
    "            print(f\"Dataset not found for event ID: {evi}\")\n",
    "            continue\n",
    "        data = np.array(dataset)\n",
    "        spt = int(dataset.attrs['p_arrival_sample'])\n",
    "        dshort = data[spt-100:spt+2900, :]\n",
    "        X.append(dshort)\n",
    "        mag = round(float(dataset.attrs['source_magnitude']), 2)\n",
    "        Y.append(mag)\n",
    "    dtfl.close()\n",
    "    return torch.tensor(np.array(X), dtype=torch.float32), torch.tensor(np.array(Y), dtype=torch.float32)\n",
    "\n",
    "def string_convertor(dd):\n",
    "    \"\"\"Convert string-format SNR values to float list.\"\"\"\n",
    "    dd2 = dd.split()\n",
    "    SNR = []\n",
    "    for d in dd2:\n",
    "        if d not in ['[', ']']:\n",
    "            dL = d.split('[')\n",
    "            dR = d.split(']')\n",
    "            if len(dL) == 2:\n",
    "                dig = dL[1]\n",
    "            elif len(dR) == 2:\n",
    "                dig = dR[0]\n",
    "            elif len(dR) == 1 and len(dL) == 1:\n",
    "                dig = d\n",
    "            try:\n",
    "                dig = float(dig)\n",
    "            except Exception:\n",
    "                dig = None\n",
    "            SNR.append(dig)\n",
    "    return SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 3: Data Loading and Initial Processing\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Define file paths\n",
    "file_name = \"merge.hdf5\"  # Replace with your path\n",
    "csv_file = \"merge.csv\"    # Replace with your path\n",
    "\n",
    "# Verify file existence\n",
    "assert os.path.isfile(file_name), f\"HDF5 file not found at {file_name}\"\n",
    "assert os.path.isfile(csv_file), f\"CSV file not found at {csv_file}\"\n",
    "\n",
    "# Load initial dataset\n",
    "df = pd.read_csv(csv_file, low_memory=False)\n",
    "print(f\"Initial number of records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Initial Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 4: Initial Data Analysis and Visualization\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Display initial value counts for trace categories\n",
    "print(\"\\nValue counts before filtering:\")\n",
    "trace_category_counts_before = df[\"trace_category\"].value_counts()\n",
    "print(trace_category_counts_before)\n",
    "\n",
    "# Plot initial magnitude distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"source_magnitude\"], bins=30, kde=True)\n",
    "plt.xlabel('Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "max_magnitude = df[\"source_magnitude\"].max()\n",
    "min_magnitude = df[\"source_magnitude\"].min()\n",
    "plt.text(6.5, 120000, f'Max: {max_magnitude:.2f} M\\nMin: {min_magnitude:.2f} M',\n",
    "         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), \n",
    "         fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Data Filtering and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 5: Data Filtering and Processing\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Convert to datetime and sort chronologically\n",
    "df['source_origin_time'] = pd.to_datetime(df['source_origin_time'])\n",
    "df = df.sort_values(by='source_origin_time')\n",
    "\n",
    "# Apply basic filters\n",
    "print(\"\\nApplying basic filters...\")\n",
    "df = df[df.trace_category == 'earthquake_local']\n",
    "df = df[df.source_distance_km <= 110]\n",
    "df = df[df.source_magnitude_type == 'ml']\n",
    "\n",
    "# Apply sample-based filters\n",
    "df = df[df.p_arrival_sample >= 200]\n",
    "df = df[df.p_arrival_sample + 2900 <= 6000]\n",
    "df = df[df.p_arrival_sample <= 1500]\n",
    "df = df[df.s_arrival_sample >= 200]\n",
    "df = df[df.s_arrival_sample <= 2500]\n",
    "\n",
    "# Process coda end samples\n",
    "print(\"\\nProcessing coda end samples...\")\n",
    "df['coda_end_sample'] = df['coda_end_sample'].apply(lambda x: float(x.strip('[]')))\n",
    "df = df.dropna(subset=['coda_end_sample'])\n",
    "df = df[df['coda_end_sample'] <= 3000]\n",
    "\n",
    "# Additional parameter filters\n",
    "print(\"\\nApplying parameter filters...\")\n",
    "df = df[df.p_travel_sec.notnull()]\n",
    "df = df[df.p_travel_sec > 0]\n",
    "df = df[df.source_distance_km.notnull()]\n",
    "df = df[df.source_distance_km > 0]\n",
    "df = df[df.source_depth_km.notnull()]\n",
    "df = df[df.source_magnitude.notnull()]\n",
    "df = df[df.back_azimuth_deg.notnull()]\n",
    "df = df[df.back_azimuth_deg > 0]\n",
    "\n",
    "# Process SNR\n",
    "print(\"\\nProcessing SNR values...\")\n",
    "df.snr_db = df.snr_db.apply(lambda x: np.mean(string_convertor(x)))\n",
    "df = df[df.snr_db >= 20]\n",
    "\n",
    "# Plot filtered magnitude distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"source_magnitude\"], bins=30, kde=True, color='brown')\n",
    "plt.xlabel('Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "max_magnitude = df[\"source_magnitude\"].max()\n",
    "min_magnitude = df[\"source_magnitude\"].min()\n",
    "plt.text(4, 35000, f'Max: {max_magnitude:.2f} M\\nMin: {min_magnitude:.2f} M',\n",
    "         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), \n",
    "         fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNumber of records after filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Multi-Observation Station Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 6: Multi-Observation Station Processing\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Identify stations with multiple observations\n",
    "print(\"\\nProcessing multi-observation stations...\")\n",
    "uniq_ins = df.receiver_code.unique()\n",
    "\n",
    "labM = []\n",
    "for ii in range(0, len(uniq_ins)):\n",
    "    station_count = sum(n == str(uniq_ins[ii]) for n in df.receiver_code)\n",
    "    print(f\"Station {str(uniq_ins[ii])}: {station_count} observations\")\n",
    "    if station_count >= 400:  # Threshold for multi-observations\n",
    "        labM.append(str(uniq_ins[ii]))\n",
    "\n",
    "print(f\"\\nNumber of multi-observation stations: {len(labM)}\")\n",
    "\n",
    "# Save the multi-observations list\n",
    "multi_observations_path = \"multi_observations.npy\"\n",
    "np.save(multi_observations_path, labM)\n",
    "\n",
    "# Load the multi-observations file for verification\n",
    "multi_observations = np.load(multi_observations_path)\n",
    "\n",
    "# Filter dataset to include only multi-observations\n",
    "ev_list = []\n",
    "for index, row in df.iterrows():\n",
    "    st = row['receiver_code']\n",
    "    if st in multi_observations:\n",
    "        ev_list.append(row['trace_name'])\n",
    "\n",
    "# Verify events exist in HDF5 file\n",
    "with h5py.File(file_name, 'r') as dtfl:\n",
    "    available_event_ids = set([key.split('/')[-1] for key in dtfl['data'].keys()])\n",
    "\n",
    "# Keep only valid events\n",
    "valid_events = [event for event in ev_list if event in available_event_ids]\n",
    "print(f\"\\nNumber of valid events: {len(valid_events)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Load Waveform Data and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 7: Load Waveform Data and Extract Features\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Load waveform data\n",
    "print(\"\\nLoading waveform data...\")\n",
    "data, labels = load_data(file_name, valid_events)\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "# Function to extract max values\n",
    "def extract_max_values(data):\n",
    "    \"\"\"Extract maximum absolute values from waveforms for each component.\"\"\"\n",
    "    return torch.max(torch.abs(data), dim=0)[0]\n",
    "\n",
    "# Function to plot waveforms\n",
    "def plot_waveform(data, p_arrival, s_arrival, coda_end, index, sampling_rate=100):\n",
    "    \"\"\"Plot three-component waveform with arrival times.\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    data = data[:3000, :]\n",
    "    time_axis = np.arange(0, data.shape[0]) / sampling_rate\n",
    "\n",
    "    for i, component in enumerate(['E-W', 'N-S', 'Vertical']):\n",
    "        ax = fig.add_subplot(3, 1, i+1)\n",
    "        ax.plot(time_axis, data[:, i], 'k')\n",
    "        ax.axvline(x=p_arrival / sampling_rate, color='b', linewidth=2, label='P-arrival')\n",
    "        ax.axvline(x=s_arrival / sampling_rate, color='r', linewidth=2, label='S-arrival')\n",
    "        ax.axvline(x=coda_end / sampling_rate, color='aqua', linewidth=2, label='Coda End')\n",
    "        ax.set_ylabel('Amplitude counts', fontweight='bold', fontsize=14)\n",
    "        ax.legend(loc='upper right', fontsize=14)\n",
    "        ax.set_title(f'Waveform {index+1}: {component} Component', \n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "        if i == 2:\n",
    "            ax.set_xlabel('Time (s)', fontweight='bold', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Extract and Analyze Maximum Amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 8: Extract and Analyze Maximum Amplitudes\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Plot example waveforms\n",
    "print(\"\\nPlotting example waveforms...\")\n",
    "with h5py.File(file_name, 'r') as dtfl:\n",
    "    for i in range(5):  # Plot first 5 waveforms\n",
    "        evi = valid_events[i]\n",
    "        dataset = dtfl.get(f'data/{evi}')\n",
    "        if dataset is None:\n",
    "            continue\n",
    "            \n",
    "        data_np = np.array(dataset)\n",
    "        p_arrival = dataset.attrs['p_arrival_sample']\n",
    "        s_arrival = dataset.attrs['s_arrival_sample']\n",
    "        coda_end = dataset.attrs['coda_end_sample']\n",
    "\n",
    "        plot_waveform(data_np, p_arrival, s_arrival, coda_end, i)\n",
    "\n",
    "        print(f\"\\nAttributes of event {i+1}:\")\n",
    "        for attr in dataset.attrs:\n",
    "            print(f\"{attr}: {dataset.attrs[attr]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Initialize lists for max values\n",
    "print(\"\\nExtracting maximum amplitudes...\")\n",
    "max_values = []\n",
    "log_max_values = []\n",
    "\n",
    "# Process each event\n",
    "for i in tqdm(range(len(data)), desc=\"Processing events\"):\n",
    "    try:\n",
    "        event_max = extract_max_values(data[i])\n",
    "        max_values.append(event_max)\n",
    "        log_event_max = torch.log10(event_max + 1e-10)  # Add small constant to avoid log(0)\n",
    "        log_max_values.append(log_event_max)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event {i}: {e}\")\n",
    "        print(f\"Shape of data[{i}]: {data[i].shape}\")\n",
    "        print(f\"Type of event_max: {type(event_max)}\")\n",
    "        print(f\"Value of event_max: {event_max}\")\n",
    "        break\n",
    "\n",
    "# Convert lists to tensors\n",
    "max_values = torch.stack(max_values)\n",
    "log_max_values = torch.stack(log_max_values)\n",
    "\n",
    "# Save the extracted values\n",
    "np.save('max_values.npy', max_values.numpy())\n",
    "np.save('log_max_values.npy', log_max_values.numpy())\n",
    "\n",
    "print(\"\\nExtraction complete. Results saved.\")\n",
    "print(f\"Max values shape: {max_values.shape}\")\n",
    "print(f\"Log max values shape: {log_max_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Statistical Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 9: Statistical Analysis and Visualization\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Analyze vertical component (index 2)\n",
    "vertical_index = 2\n",
    "\n",
    "# Plot histograms of max values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(max_values[:, vertical_index].numpy(), bins=50)\n",
    "plt.xlabel('Stream Max Value', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(log_max_values[:, vertical_index].numpy(), bins=50)\n",
    "plt.xlabel('Log Stream Max Value', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "correlation = np.corrcoef(max_values[:, vertical_index].numpy(), \n",
    "                         labels.numpy())[0, 1]\n",
    "correlation_log = np.corrcoef(log_max_values[:, vertical_index].numpy(), \n",
    "                            labels.numpy())[0, 1]\n",
    "\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "print(f\"Correlation between vertical stream max amplitude and magnitude: {correlation:.4f}\")\n",
    "print(f\"Correlation between vertical log stream max amplitude and magnitude: {correlation_log:.4f}\")\n",
    "\n",
    "# Scatter plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(labels.numpy(), max_values[:, vertical_index].numpy(), \n",
    "                     alpha=0.5, c=labels.numpy(), cmap='viridis')\n",
    "plt.xlabel('Earthquake Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Stream Max Amplitude (Vertical)', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Density', fontweight='bold', fontsize=14)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter_log = plt.scatter(labels.numpy(), log_max_values[:, vertical_index].numpy(), \n",
    "                         alpha=0.5, c=labels.numpy(), cmap='viridis')\n",
    "plt.xlabel('Earthquake Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Log Stream Max Amplitude (Vertical)', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "cbar = plt.colorbar(scatter_log)\n",
    "cbar.set_label('Density', fontweight='bold', fontsize=14)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Save Final Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 10: Save Final Processed Data\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Save the final preprocessed data\n",
    "output_data_file = \"pre_processed_data.npy\"\n",
    "output_labels_file = \"pre_processed_labels.npy\"\n",
    "\n",
    "np.save(output_data_file, data.cpu().numpy())\n",
    "np.save(output_labels_file, labels.cpu().numpy())\n",
    "\n",
    "print(f\"\\nPre-processed Data saved to {output_data_file}\")\n",
    "print(f\"Pre-processed Labels saved to {output_labels_file}\")\n",
    "\n",
    "# Final timing information\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal execution time: {elapsed_time/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
