{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Title: Enhanced Deeper Architecture for Earthquake Magnitude Estimation\n",
    "\n",
    "This notebook implements an enhanced deeper architecture building upon the original \n",
    "MagNet model. Key improvements include:\n",
    "1. Additional convolutional layers with residual connections \n",
    "2. Dual LSTM layers\n",
    "3. Integration of log-transformed stream max amplitude information\n",
    "4. Enhanced uncertainty quantification\n",
    "\n",
    "Dependencies:\n",
    "- torch, torchinfo \n",
    "- numpy, matplotlib\n",
    "- tqdm (for progress tracking)\n",
    "- seaborn (for visualization)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 1: Setup and Imports \n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from torchinfo import summary\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Configure environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 2: Dataset Implementation\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    \"\"\"Dataset class handling waveforms, labels, and log stream max values.\"\"\"\n",
    "    def __init__(self, data, labels, log_stream_max):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.log_stream_max = log_stream_max\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.log_stream_max[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Architecture  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 3: Model Architecture  \n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with batch normalization.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Downsample if needed\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class EarthquakeModel(nn.Module):\n",
    "    \"\"\"Enhanced architecture with residual connections and dual LSTM.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EarthquakeModel, self).__init__()\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.res3 = ResidualBlock(128, 128)\n",
    "        self.res4 = ResidualBlock(128, 64)\n",
    "        self.res5 = ResidualBlock(64, 32)\n",
    "        \n",
    "        # Pooling and regularization\n",
    "        self.maxpool = nn.MaxPool1d(2, padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(32, 100, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(200, 200, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Output layer (including log stream max)\n",
    "        self.fc = nn.Linear(400 + 3, 2)  # 400 from LSTM, 3 from log stream max\n",
    "\n",
    "    def forward(self, x, log_stream_max):\n",
    "        # CNN processing\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.res3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.res5(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # LSTM processing\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Combine with log stream max\n",
    "        combined = torch.cat((x, log_stream_max), dim=1)\n",
    "        x = self.fc(combined)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 4: Training Components\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    \"\"\"Custom loss combining prediction error and uncertainty.\"\"\"\n",
    "    y_hat = y_pred[:, 0]\n",
    "    s = y_pred[:, 1]\n",
    "    return torch.mean(0.5 * torch.exp(-s) * (y_true - y_hat)**2 + 0.5 * s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 5: Training Function\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=300, patience=50, model_path='best_model.pth'):\n",
    "    \"\"\"Train model with early stopping and learning rate scheduling.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=15, verbose=True, min_lr=0.5e-6\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=model_path)\n",
    "    criterion = custom_loss\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, log_stream_max, target in train_loader:\n",
    "            data = data.to(device)\n",
    "            log_stream_max = log_stream_max.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data, log_stream_max)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, log_stream_max, target in val_loader:\n",
    "                data = data.to(device)\n",
    "                log_stream_max = log_stream_max.to(device)  \n",
    "                target = target.to(device)\n",
    "                outputs = model(data, log_stream_max)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        running_loss /= len(train_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Training Loss: {running_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        train_losses.append(running_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 6: Uncertainty Estimation\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def estimate_uncertainty(model, data_loader, num_samples=50):\n",
    "    \"\"\"Estimate model uncertainties using Monte Carlo dropout.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()\n",
    "\n",
    "    predictions = []\n",
    "    log_variances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            batch_predictions = []\n",
    "            batch_log_variances = []\n",
    "            for data, log_stream_max, _ in data_loader:\n",
    "                data = data.to(device)\n",
    "                log_stream_max = log_stream_max.to(device)\n",
    "                output = model(data, log_stream_max)\n",
    "                batch_predictions.append(output[:, 0].cpu().numpy())\n",
    "                batch_log_variances.append(output[:, 1].cpu().numpy())\n",
    "            predictions.append(np.concatenate(batch_predictions))\n",
    "            log_variances.append(np.concatenate(batch_log_variances))\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    log_variances = np.array(log_variances)\n",
    "\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    yhat_squared_mean = np.mean(np.square(predictions), axis=0)\n",
    "\n",
    "    sigma_squared = np.power(10, log_variances)\n",
    "    aleatoric_uncertainty = np.mean(sigma_squared, axis=0)\n",
    "\n",
    "    epistemic_uncertainty = np.std(predictions, axis=0)\n",
    "    combined_uncertainty = yhat_squared_mean - np.square(mean_prediction) + aleatoric_uncertainty\n",
    "\n",
    "    return mean_prediction, epistemic_uncertainty, aleatoric_uncertainty, combined_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 7: Evaluation Function\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model(model, test_loader, model_path='best_model.pth'):\n",
    "    \"\"\"Evaluate model performance and uncertainties.\"\"\"\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    mean_pred, epistemic_unc, aleatoric_unc, combined_unc = estimate_uncertainty(model, test_loader)\n",
    "\n",
    "    true_values = []\n",
    "    for _, _, target in test_loader:\n",
    "        true_values.append(target.numpy())\n",
    "    true_values = np.concatenate(true_values)\n",
    "\n",
    "    mae = np.mean(np.abs(mean_pred - true_values))\n",
    "\n",
    "    return mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 8: Visualization Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def plot_uncertainties(true_values, predictions, epistemic_unc, aleatoric_unc, combined_unc, seed):\n",
    "    \"\"\"Plot uncertainty analyses and model predictions.\"\"\"\n",
    "    # Calculate thresholds\n",
    "    aleatoric_threshold = np.percentile(aleatoric_unc, 99.995)\n",
    "    epistemic_threshold = np.percentile(epistemic_unc, 99.995)\n",
    "    combined_threshold = np.percentile(combined_unc, 99.995)\n",
    "\n",
    "    # Aleatoric Uncertainty\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = aleatoric_unc < aleatoric_threshold\n",
    "    plt.errorbar(predictions[mask], aleatoric_unc[mask], \n",
    "                xerr=aleatoric_unc[mask], fmt='o', alpha=0.4, \n",
    "                ecolor='g', capthick=2)\n",
    "    plt.plot(true_values[mask], aleatoric_unc[mask], 'ro', alpha=0.4)\n",
    "    plt.xlabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Aleatoric Uncertainty', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, aleatoric_threshold)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'aleatoric_uncertainty_seed_{seed}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Epistemic Uncertainty\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = epistemic_unc < epistemic_threshold\n",
    "    plt.errorbar(predictions[mask], epistemic_unc[mask], \n",
    "                xerr=epistemic_unc[mask], fmt='o', alpha=0.4, \n",
    "                ecolor='g', capthick=2)\n",
    "    plt.plot(true_values[mask], epistemic_unc[mask], 'ro', alpha=0.4)\n",
    "    plt.xlabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Epistemic Uncertainty', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, epistemic_threshold)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'epistemic_uncertainty_seed_{seed}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Combined Uncertainty\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = combined_unc < combined_threshold\n",
    "    plt.errorbar(predictions[mask], combined_unc[mask], \n",
    "                xerr=combined_unc[mask], fmt='o', alpha=0.4, \n",
    "                ecolor='g', capthick=2)\n",
    "    plt.plot(true_values[mask], combined_unc[mask], 'ro', alpha=0.4)\n",
    "    plt.xlabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Combined Uncertainty', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, combined_threshold)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'combined_uncertainty_seed_{seed}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Predicted vs True\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(true_values, predictions, alpha=0.4, facecolors='none', edgecolors='r')\n",
    "    plt.plot([true_values.min(), true_values.max()], \n",
    "             [true_values.min(), true_values.max()], \n",
    "             'k--', alpha=0.4, lw=2)\n",
    "    plt.xlabel('True Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'predicted_vs_true_seed_{seed}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 9: Main Execution\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 1024\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Load preprocessed data\n",
    "    output_data_file = \"pre_processed_data.npy\"\n",
    "    output_labels_file = \"pre_processed_labels.npy\"\n",
    "    log_stream_max_file = \"log_max_values.npy\"\n",
    "\n",
    "    # Verify file existence\n",
    "    assert os.path.isfile(output_data_file), f\"Data file not found at {output_data_file}\"\n",
    "    assert os.path.isfile(output_labels_file), f\"Labels file not found at {output_labels_file}\"\n",
    "    assert os.path.isfile(log_stream_max_file), f\"Log stream max file not found at {log_stream_max_file}\"\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    data = torch.tensor(np.load(output_data_file), dtype=torch.float32)\n",
    "    labels = torch.tensor(np.load(output_labels_file), dtype=torch.float32)\n",
    "    log_stream_max = torch.tensor(np.load(log_stream_max_file), dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Data shapes:\")\n",
    "    print(f\"Waveforms: {data.shape}\")\n",
    "    print(f\"Labels: {labels.shape}\")\n",
    "    print(f\"Log stream max: {log_stream_max.shape}\")\n",
    "\n",
    "    # Split data into train/validation/test (80/10/10)\n",
    "    total_samples = len(data)\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    val_size = int(0.1 * total_samples)\n",
    "    test_size = total_samples - train_size - val_size\n",
    "\n",
    "    print(f\"\\nData splitting:\")\n",
    "    print(f\"Training samples: {train_size}\")\n",
    "    print(f\"Validation samples: {val_size}\")\n",
    "    print(f\"Testing samples: {test_size}\")\n",
    "\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "\n",
    "    train_labels = labels[:train_size]\n",
    "    val_labels = labels[train_size:train_size + val_size]\n",
    "    test_labels = labels[train_size + val_size:]\n",
    "\n",
    "    train_log_stream_max = log_stream_max[:train_size]\n",
    "    val_log_stream_max = log_stream_max[train_size:train_size + val_size]\n",
    "    test_log_stream_max = log_stream_max[train_size + val_size:]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = EarthquakeDataset(train_data, train_labels, train_log_stream_max)\n",
    "    val_dataset = EarthquakeDataset(val_data, val_labels, val_log_stream_max)\n",
    "    test_dataset = EarthquakeDataset(test_data, test_labels, test_log_stream_max)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Initialize model\n",
    "    model = EarthquakeModel()\n",
    "    print(\"\\nModel architecture:\")\n",
    "    print(summary(model, input_size=[(256, 3000, 3), (256, 3)]))\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    model_path = f'best_model_seed_{seed}.pth'\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader,\n",
    "                                         num_epochs=300, patience=50,\n",
    "                                         model_path=model_path)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc = \\\n",
    "        evaluate_model(model, test_loader, model_path=model_path)\n",
    "\n",
    "    # Print results\n",
    "    print(f'\\nFinal Results:')\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'Mean Aleatoric Uncertainty: {np.mean(aleatoric_unc):.4f}')\n",
    "    print(f'Mean Epistemic Uncertainty: {np.mean(epistemic_unc):.4f}')\n",
    "    print(f'Mean Combined Uncertainty: {np.mean(combined_unc):.4f}')\n",
    "\n",
    "    # Plot results\n",
    "    print(\"\\nGenerating plots...\")\n",
    "    plot_uncertainties(true_values, mean_pred, epistemic_unc, aleatoric_unc, combined_unc, seed)\n",
    "\n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'loss_curves_seed_{seed}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save results to JSON\n",
    "    results = {\n",
    "        \"seed\": seed,\n",
    "        \"mae\": float(mae),\n",
    "        \"mean_aleatoric_uncertainty\": float(np.mean(aleatoric_unc)),\n",
    "        \"mean_epistemic_uncertainty\": float(np.mean(epistemic_unc)),\n",
    "        \"mean_combined_uncertainty\": float(np.mean(combined_unc))\n",
    "    }\n",
    "\n",
    "    with open(f'model_results_seed_{seed}.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {(end_time - start_time)/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
