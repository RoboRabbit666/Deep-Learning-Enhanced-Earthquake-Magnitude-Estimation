{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Title: Statistical Analysis of Data Splitting Strategies in Magnitude Estimation\n",
    "\n",
    "This notebook conducts a comprehensive analysis of data splitting strategies by:\n",
    "1. Running 50 experiments with random splits\n",
    "2. Comparing with chronological splitting\n",
    "3. Analyzing statistical significance of results\n",
    "4. Investigating evidence of data leakage\n",
    "\n",
    "Dependencies:\n",
    "- torch, numpy, json\n",
    "- matplotlib, seaborn (for visualization)\n",
    "- tqdm (for progress tracking)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 1: Setup and Imports\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dataset and Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 2: Dataset and Model Classes\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    \"\"\"Dataset class for earthquake data.\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class EarthquakeModel(nn.Module):\n",
    "    \"\"\"Original MagNet architecture for comparison purposes.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EarthquakeModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool1d(4, padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(32, 100, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 3: Training Components\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, run_id=None, \n",
    "                 test_seed=None, model_seed=None):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.run_id = run_id\n",
    "        self.test_seed = test_seed\n",
    "        self.model_seed = model_seed\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "        model_filename = f'best_model_Run_{self.run_id}_test_data_seed_{self.test_seed}_model_seed_{self.model_seed}.pth'\n",
    "        torch.save(model.state_dict(), model_filename)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    \"\"\"Custom loss function combining prediction error and uncertainty.\"\"\"\n",
    "    y_hat = y_pred[:, 0]\n",
    "    s = y_pred[:, 1]\n",
    "    return torch.mean(0.5 * torch.exp(-s) * (y_true - y_hat)**2 + 0.5 * s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 4: Training and Evaluation Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=300, patience=5, \n",
    "                run_id=None, test_seed=None, model_seed=None):\n",
    "    \"\"\"Train the model with early stopping and learning rate scheduling.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=np.sqrt(0.1),\n",
    "        cooldown=0, patience=4, verbose=True, min_lr=0.5e-6\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=patience, verbose=True,\n",
    "        run_id=run_id, test_seed=test_seed, model_seed=model_seed\n",
    "    )\n",
    "    \n",
    "    criterion = custom_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        val_loss /= len(val_loader)\n",
    "        running_loss /= len(train_loader)\n",
    "\n",
    "        # Learning rate scheduling and early stopping\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, '\n",
    "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        train_losses.append(running_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def estimate_uncertainty(model, data_loader, num_samples=50):\n",
    "    \"\"\"Estimate model uncertainty using Monte Carlo dropout.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Enable dropout during inference\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()\n",
    "\n",
    "    predictions = []\n",
    "    log_variances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            batch_predictions = []\n",
    "            batch_log_variances = []\n",
    "            for data, _ in data_loader:\n",
    "                data = data.to(device)\n",
    "                output = model(data)\n",
    "                batch_predictions.append(output[:, 0].cpu().numpy())\n",
    "                batch_log_variances.append(output[:, 1].cpu().numpy())\n",
    "            predictions.append(np.concatenate(batch_predictions))\n",
    "            log_variances.append(np.concatenate(batch_log_variances))\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    log_variances = np.array(log_variances)\n",
    "\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    yhat_squared_mean = np.mean(np.square(predictions), axis=0)\n",
    "\n",
    "    sigma_squared = np.power(10, log_variances)\n",
    "    aleatoric_uncertainty = np.mean(sigma_squared, axis=0)\n",
    "\n",
    "    epistemic_uncertainty = np.std(predictions, axis=0)\n",
    "    combined_uncertainty = yhat_squared_mean - np.square(mean_prediction) + aleatoric_uncertainty\n",
    "\n",
    "    return mean_prediction, epistemic_uncertainty, aleatoric_uncertainty, combined_uncertainty\n",
    "\n",
    "def evaluate_model(model, test_loader, run_id, test_seed, model_seed):\n",
    "    \"\"\"Evaluate model performance and uncertainties.\"\"\"\n",
    "    model_filename = f'best_model_Run_{run_id}_test_data_seed_{test_seed}_model_seed_{model_seed}.pth'\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "    \n",
    "    mean_pred, epistemic_unc, aleatoric_unc, combined_unc = estimate_uncertainty(model, test_loader)\n",
    "\n",
    "    true_values = []\n",
    "    for _, target in test_loader:\n",
    "        true_values.append(target.numpy())\n",
    "    true_values = np.concatenate(true_values)\n",
    "\n",
    "    mae = np.mean(np.abs(mean_pred - true_values))\n",
    "\n",
    "    return mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Experimental Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 5: Experimental Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def run_experiment(data, labels, test_seed, model_seed, run_id):\n",
    "    \"\"\"Run a single experimental iteration.\"\"\"\n",
    "    set_seed(test_seed)\n",
    "\n",
    "    # Split data (70-10-20)\n",
    "    train_val_data, test_data, train_val_labels, test_labels = train_test_split(\n",
    "        data, labels, test_size=0.2, shuffle=True)\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_val_data, train_val_labels, test_size=0.125, shuffle=True)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = EarthquakeDataset(train_data, train_labels)\n",
    "    val_dataset = EarthquakeDataset(val_data, val_labels)\n",
    "    test_dataset = EarthquakeDataset(test_data, test_labels)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Initialize and train model\n",
    "    set_seed(model_seed)\n",
    "    model = EarthquakeModel().to(device)\n",
    "    \n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        run_id=run_id, test_seed=test_seed, model_seed=model_seed\n",
    "    )\n",
    "    \n",
    "    mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc = \\\n",
    "        evaluate_model(model, test_loader, run_id, test_seed, model_seed)\n",
    "\n",
    "    return {\n",
    "        \"test_seed\": test_seed,\n",
    "        \"model_seed\": model_seed,\n",
    "        \"mae\": float(mae),\n",
    "        \"mean_aleatoric_uncertainty\": float(np.mean(aleatoric_unc)),\n",
    "        \"mean_epistemic_uncertainty\": float(np.mean(epistemic_unc)),\n",
    "        \"mean_combined_uncertainty\": float(np.mean(combined_unc))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 6: Visualization Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def plot_experimental_results(results):\n",
    "    \"\"\"Plot comprehensive analysis of experimental results.\"\"\"\n",
    "    # Extract metrics\n",
    "    mae_values = [result['median_mae'] for result in results]\n",
    "    aleatoric_values = [result['median_aleatoric_uncertainty'] for result in results]\n",
    "    epistemic_values = [result['median_epistemic_uncertainty'] for result in results]\n",
    "    combined_values = [result['median_combined_uncertainty'] for result in results]\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_mae = np.mean(mae_values)\n",
    "    std_mae = np.std(mae_values)\n",
    "\n",
    "    # Plot 1: MAE Distribution\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    n, bins, patches = plt.hist(mae_values, bins=10, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.axvline(mean_mae, color='red', linestyle='dashed', linewidth=2,\n",
    "                label=f'Mean = {mean_mae:.4f}')\n",
    "    plt.axvline(mean_mae + std_mae, color='green', linestyle='dotted', linewidth=2,\n",
    "                label=f'Mean ± Std = {mean_mae + std_mae:.4f}')\n",
    "    plt.axvline(mean_mae - std_mae, color='blue', linestyle='dotted', linewidth=2,\n",
    "                label=f'Mean - Std = {mean_mae - std_mae:.4f}')\n",
    "    \n",
    "    plt.annotate(f'0.2441: test MAE\\n(chronological splitting)',\n",
    "                xy=(0.2441, 0), xytext=(0.2530, max(n) * 0.6),\n",
    "                arrowprops=dict(facecolor='orange', shrink=0),\n",
    "                fontsize=18, ha='right', va='center')\n",
    "    \n",
    "    plt.xlabel('MAE', fontsize=18, fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontsize=18, fontweight='bold')\n",
    "    plt.title('MAE Distribution over 50 Random Splitting Runs', \n",
    "              fontsize=18, fontweight='bold')\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mae_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Uncertainty Distributions\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 24))\n",
    "    \n",
    "    # Aleatoric Uncertainty\n",
    "    sns.histplot(aleatoric_values, bins=10, ax=axes[0], alpha=0.7, \n",
    "                edgecolor='black')\n",
    "    axes[0].axvline(np.mean(aleatoric_values), color='red', linestyle='--',\n",
    "                    label=f'Mean = {np.mean(aleatoric_values):.4f}')\n",
    "    axes[0].set_title('Aleatoric Uncertainty Distribution', \n",
    "                      fontsize=18, fontweight='bold')\n",
    "    axes[0].set_xlabel('Mean Aleatoric Uncertainty', fontsize=18, fontweight='bold')\n",
    "    axes[0].set_ylabel('Frequency', fontsize=18, fontweight='bold')\n",
    "    axes[0].tick_params(labelsize=18)\n",
    "    axes[0].legend(fontsize=18)\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Epistemic Uncertainty\n",
    "    sns.histplot(epistemic_values, bins=10, ax=axes[1], alpha=0.7, \n",
    "                edgecolor='black')\n",
    "    axes[1].axvline(np.mean(epistemic_values), color='red', linestyle='--',\n",
    "                    label=f'Mean = {np.mean(epistemic_values):.4f}')\n",
    "    axes[1].set_title('Epistemic Uncertainty Distribution', \n",
    "                      fontsize=18, fontweight='bold')\n",
    "    axes[1].set_xlabel('Mean Epistemic Uncertainty', fontsize=18, fontweight='bold')\n",
    "    axes[1].set_ylabel('Frequency', fontsize=18, fontweight='bold')\n",
    "    axes[1].tick_params(labelsize=18)\n",
    "    axes[1].legend(fontsize=18)\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Combined Uncertainty\n",
    "    sns.histplot(combined_values, bins=10, ax=axes[2], alpha=0.7, \n",
    "                edgecolor='black')\n",
    "    axes[2].axvline(np.mean(combined_values), color='red', linestyle='--',\n",
    "                    label=f'Mean = {np.mean(combined_values):.4f}')\n",
    "    axes[2].set_title('Combined Uncertainty Distribution', \n",
    "                      fontsize=18, fontweight='bold')\n",
    "    axes[2].set_xlabel('Mean Combined Uncertainty', fontsize=18, fontweight='bold')\n",
    "    axes[2].set_ylabel('Frequency', fontsize=18, fontweight='bold')\n",
    "    axes[2].tick_params(labelsize=18)\n",
    "    axes[2].legend(fontsize=18)\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('uncertainty_distributions.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: Box Plot of MAEs\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.boxplot(mae_values)\n",
    "    plt.ylabel('MAE', fontsize=18, fontweight='bold')\n",
    "    plt.title('Box Plot of MAEs across 50 Runs', fontsize=18, fontweight='bold')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mae_boxplot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 7: Main Execution\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load preprocessed data\n",
    "    output_data_file = \"pre_processed_data.npy\"\n",
    "    output_labels_file = \"pre_processed_labels.npy\"\n",
    "\n",
    "    # Verify files exist\n",
    "    assert os.path.isfile(output_data_file), f\"Data file not found at {output_data_file}\"\n",
    "    assert os.path.isfile(output_labels_file), f\"Labels file not found at {output_labels_file}\"\n",
    "\n",
    "    # Load data\n",
    "    data = torch.tensor(np.load(output_data_file), dtype=torch.float32)\n",
    "    labels = torch.tensor(np.load(output_labels_file), dtype=torch.float32)\n",
    "    print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    model_seeds = [42, 123, 256, 789, 1024]  # 5 different model initializations\n",
    "    results_file = \"results_50_runs.json\"\n",
    "\n",
    "    # Run experiments\n",
    "    for run_id in tqdm(range(1, 51)):  # 50 different test sets\n",
    "        test_results = []\n",
    "        for model_seed in model_seeds:\n",
    "            result = run_experiment(data, labels, run_id, model_seed, run_id)\n",
    "            test_results.append(result)\n",
    "\n",
    "        # Find median performance\n",
    "        sorted_results = sorted(test_results, key=lambda x: x['mae'])\n",
    "        median_result = sorted_results[2]  # Index 2 is median of 5\n",
    "\n",
    "        results.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"median_mae\": median_result['mae'],\n",
    "            \"median_aleatoric_uncertainty\": median_result['mean_aleatoric_uncertainty'],\n",
    "            \"median_epistemic_uncertainty\": median_result['mean_epistemic_uncertainty'],\n",
    "            \"median_combined_uncertainty\": median_result['mean_combined_uncertainty'],\n",
    "            \"all_results\": test_results\n",
    "        })\n",
    "\n",
    "        # Save results after each run\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "        print(f\"\\nCompleted run {run_id}/50\")\n",
    "        print(f\"Median MAE: {median_result['mae']:.4f}\")\n",
    "        print(f\"Median Aleatoric Uncertainty: {median_result['mean_aleatoric_uncertainty']:.4f}\")\n",
    "        print(f\"Median Epistemic Uncertainty: {median_result['mean_epistemic_uncertainty']:.4f}\")\n",
    "        print(f\"Median Combined Uncertainty: {median_result['mean_combined_uncertainty']:.4f}\")\n",
    "\n",
    "    # Plot final results\n",
    "    print(\"\\nGenerating final plots...\")\n",
    "    plot_experimental_results(results)\n",
    "\n",
    "    # Calculate overall statistics\n",
    "    maes = [result[\"median_mae\"] for result in results]\n",
    "    mean_mae = np.mean(maes)\n",
    "    std_mae = np.std(maes)\n",
    "\n",
    "    print(\"\\nFinal Statistics:\")\n",
    "    print(f\"Mean MAE: {mean_mae:.4f}\")\n",
    "    print(f\"Standard Deviation of MAE: {std_mae:.4f}\")\n",
    "    print(f\"Minimum MAE: {min(maes):.4f}\")\n",
    "    print(f\"Maximum MAE: {max(maes):.4f}\")\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\nTotal execution time: {elapsed_time/60:.2f} minutes\")\n",
    "\n",
    "    print(\"\\nExperiment completed. Results saved in 'results_50_runs.json'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
