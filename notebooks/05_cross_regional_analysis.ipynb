{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Cross-Regional Adaptation Studies in Earthquake Magnitude Estimation\n",
    "\n",
    "This notebook investigates transfer learning effectiveness for adapting magnitude \n",
    "estimation models across geological regions. Key components:\n",
    "1. Performance evaluation on out-of-distribution regional data\n",
    "2. Transfer learning implementation \n",
    "3. Comparative analysis with baseline performance\n",
    "\n",
    "Dependencies:\n",
    "- torch, numpy, json\n",
    "- matplotlib (for visualization)\n",
    "- h5py (for seismic data loading)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 1: Setup and Imports\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from torchinfo import summary\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 2: Dataset Class\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    \"\"\"Dataset class handling waveforms, labels, and log stream max values.\"\"\"\n",
    "    def __init__(self, data, labels, log_stream_max):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.log_stream_max = log_stream_max\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.log_stream_max[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 3: Model Components\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with batch normalization.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class EarthquakeModel(nn.Module):\n",
    "    \"\"\"Enhanced architecture with residual connections and log stream max.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EarthquakeModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.res3 = ResidualBlock(128, 128)\n",
    "        self.res4 = ResidualBlock(128, 64)\n",
    "        self.res5 = ResidualBlock(64, 32)\n",
    "        self.maxpool = nn.MaxPool1d(2, padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm1 = nn.LSTM(32, 100, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(200, 200, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(400 + 3, 2)\n",
    "\n",
    "    def forward(self, x, log_stream_max):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.res5(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        combined = torch.cat((x, log_stream_max), dim=1)\n",
    "        x = self.fc(combined)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 4: Training Components\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping with model checkpointing.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    \"\"\"Custom loss combining prediction error and uncertainty.\"\"\"\n",
    "    y_hat = y_pred[:, 0]\n",
    "    s = y_pred[:, 1]\n",
    "    return torch.mean(0.5 * torch.exp(-s) * (y_true - y_hat)**2 + 0.5 * s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Transfer Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 5: Transfer Learning Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def transfer_learning(model, train_loader, val_loader, num_epochs=300, patience=50, \n",
    "                     model_path='transfer_learning_model.pth'):\n",
    "    \"\"\"Implement transfer learning by freezing convolutional layers.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Freeze convolutional layers\n",
    "    for param in model.conv1.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.conv2.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.res3.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.res4.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.res5.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=15, verbose=True, min_lr=0.5e-6)\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=model_path)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, log_stream_max, target in train_loader:\n",
    "            data = data.to(device)\n",
    "            log_stream_max = log_stream_max.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data, log_stream_max)\n",
    "            loss = custom_loss(outputs, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, log_stream_max, target in val_loader:\n",
    "                data = data.to(device)\n",
    "                log_stream_max = log_stream_max.to(device)\n",
    "                target = target.to(device)\n",
    "                outputs = model(data, log_stream_max)\n",
    "                loss = custom_loss(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        running_loss /= len(train_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        train_losses.append(running_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 6: Evaluation Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def estimate_uncertainty(model, data_loader, num_samples=50):\n",
    "    \"\"\"Estimate model uncertainties using Monte Carlo dropout.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()\n",
    "\n",
    "    predictions = []\n",
    "    log_variances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            batch_predictions = []\n",
    "            batch_log_variances = []\n",
    "            for data, log_stream_max, _ in data_loader:\n",
    "                data = data.to(device)\n",
    "                log_stream_max = log_stream_max.to(device)\n",
    "                output = model(data, log_stream_max)\n",
    "                batch_predictions.append(output[:, 0].cpu().numpy())\n",
    "                batch_log_variances.append(output[:, 1].cpu().numpy())\n",
    "            predictions.append(np.concatenate(batch_predictions))\n",
    "            log_variances.append(np.concatenate(batch_log_variances))\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    log_variances = np.array(log_variances)\n",
    "\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    yhat_squared_mean = np.mean(np.square(predictions), axis=0)\n",
    "\n",
    "    sigma_squared = np.power(10, log_variances)\n",
    "    aleatoric_uncertainty = np.mean(sigma_squared, axis=0)\n",
    "\n",
    "    epistemic_uncertainty = np.std(predictions, axis=0)\n",
    "    combined_uncertainty = yhat_squared_mean - np.square(mean_prediction) + aleatoric_uncertainty\n",
    "\n",
    "    return mean_prediction, epistemic_uncertainty, aleatoric_uncertainty, combined_uncertainty\n",
    "\n",
    "def evaluate_model(model, test_loader, model_path='transfer_learning_model.pth'):\n",
    "    \"\"\"Evaluate model performance and uncertainties.\"\"\"\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    mean_pred, epistemic_unc, aleatoric_unc, combined_unc = estimate_uncertainty(model, test_loader)\n",
    "\n",
    "    true_values = []\n",
    "    for _, _, target in test_loader:\n",
    "        true_values.append(target.numpy())\n",
    "    true_values = np.concatenate(true_values)\n",
    "\n",
    "    mae = np.mean(np.abs(mean_pred - true_values))\n",
    "\n",
    "    return mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 7: Visualization Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def plot_uncertainties(true_values, predictions, epistemic_unc, aleatoric_unc, \n",
    "                      combined_unc, seed):\n",
    "    \"\"\"Plot uncertainty distributions and prediction results.\"\"\"\n",
    "    aleatoric_threshold = np.percentile(aleatoric_unc, 99.995)\n",
    "    epistemic_threshold = np.percentile(epistemic_unc, 99.995)\n",
    "    combined_threshold = np.percentile(combined_unc, 99.995)\n",
    "\n",
    "    # Aleatoric Uncertainty\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = aleatoric_unc < aleatoric_threshold\n",
    "    plt.errorbar(predictions[mask], aleatoric_unc[mask], \n",
    "                xerr=aleatoric_unc[mask], fmt='o', alpha=0.4, \n",
    "                ecolor='g', capthick=2)\n",
    "    plt.plot(true_values[mask], aleatoric_unc[mask], 'ro', alpha=0.4)\n",
    "    plt.xlabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Aleatoric Uncertainty', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, aleatoric_threshold)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'aleatoric_uncertainty_seed_{seed}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Epistemic Uncertainty\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = epistemic_unc < epistemic_threshold\n",
    "    plt.errorbar(predictions[mask], epistemic_unc[mask], \n",
    "                xerr=epistemic_unc[mask], fmt='o', alpha=0.4, \n",
    "                ecolor='g', capthick=2)\n",
    "    plt.plot(true_values[mask], epistemic_unc[mask], 'ro', alpha=0.4)\n",
    "    plt.xlabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Epistemic Uncertainty', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, epistemic_threshold)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'epistemic_uncertainty_seed_{seed}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Combined Uncertainty\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = combined_unc < combined_threshold\n",
    "    plt.errorbar(predictions[mask], combined_unc[mask], \n",
    "                xerr=combined_unc[mask], fmt='o', alpha=0.4, \n",
    "                ecolor='g', capthick=2)\n",
    "    plt.plot(true_values[mask], combined_unc[mask], 'ro', alpha=0.4)\n",
    "    plt.xlabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Combined Uncertainty', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, combined_threshold)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'combined_uncertainty_seed_{seed}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Predicted vs True\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(true_values, predictions, alpha=0.4, \n",
    "               facecolors='none', edgecolors='r')\n",
    "    plt.plot([true_values.min(), true_values.max()], \n",
    "             [true_values.min(), true_values.max()], \n",
    "             'k--', alpha=0.4, lw=2)\n",
    "    plt.xlabel('True Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Predicted Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'predicted_vs_true_seed_{seed}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Part 8: Main Execution\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Set random seed\n",
    "    seed = 1024\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Define file paths\n",
    "    train_val_data_file = \"TL_MO_SCEDC_pre_processed_data_train_val.npy\"\n",
    "    train_val_labels_file = \"TL_MO_SCEDC_pre_processed_labels_train_val.npy\"\n",
    "    train_val_log_stream_max_file = \"TL_MO_SCEDC_log_stream_max_train_val.npy\"\n",
    "    \n",
    "    test_data_file = \"TL_MO_SCEDC_pre_processed_data_test.npy\"\n",
    "    test_labels_file = \"TL_MO_SCEDC_pre_processed_labels_test.npy\"\n",
    "    test_log_stream_max_file = \"TL_MO_SCEDC_log_stream_max_test.npy\"\n",
    "\n",
    "    # Verify files exist\n",
    "    for file_path in [train_val_data_file, train_val_labels_file, \n",
    "                     train_val_log_stream_max_file, test_data_file, \n",
    "                     test_labels_file, test_log_stream_max_file]:\n",
    "        assert os.path.isfile(file_path), f\"File not found at {file_path}\"\n",
    "\n",
    "    # Load train/validation data\n",
    "    print(\"\\nLoading train/validation data...\")\n",
    "    train_val_data = np.load(train_val_data_file)\n",
    "    train_val_data = train_val_data.transpose(0, 2, 1)\n",
    "    train_val_data = torch.tensor(train_val_data, dtype=torch.float32)\n",
    "\n",
    "    train_val_labels = torch.tensor(np.load(train_val_labels_file), \n",
    "                                  dtype=torch.float32)\n",
    "    train_val_log_stream_max = torch.tensor(np.load(train_val_log_stream_max_file), \n",
    "                                          dtype=torch.float32)\n",
    "\n",
    "    # Load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "    test_data = np.load(test_data_file)\n",
    "    test_data = test_data.transpose(0, 2, 1)\n",
    "    test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "    test_labels = torch.tensor(np.load(test_labels_file), dtype=torch.float32)\n",
    "    test_log_stream_max = torch.tensor(np.load(test_log_stream_max_file), \n",
    "                                     dtype=torch.float32)\n",
    "\n",
    "    # Print data shapes\n",
    "    print(f\"\\nData Shapes:\")\n",
    "    print(f\"Train/Val Data: {train_val_data.shape}\")\n",
    "    print(f\"Train/Val Labels: {train_val_labels.shape}\")\n",
    "    print(f\"Train/Val Log Stream Max: {train_val_log_stream_max.shape}\")\n",
    "    print(f\"Test Data: {test_data.shape}\")\n",
    "    print(f\"Test Labels: {test_labels.shape}\")\n",
    "    print(f\"Test Log Stream Max: {test_log_stream_max.shape}\")\n",
    "\n",
    "    # Split train/validation data\n",
    "    total_samples = len(train_val_data)\n",
    "    train_size = int(8/9 * total_samples)\n",
    "    val_size = total_samples - train_size\n",
    "\n",
    "    print(f\"\\nSplitting train/val data:\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Training samples: {train_size}\")\n",
    "    print(f\"Validation samples: {val_size}\")\n",
    "\n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data = train_val_data[train_size:]\n",
    "    train_labels = train_val_labels[:train_size]\n",
    "    val_labels = train_val_labels[train_size:]\n",
    "    train_log_stream_max = train_val_log_stream_max[:train_size]\n",
    "    val_log_stream_max = train_val_log_stream_max[train_size:]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = EarthquakeDataset(train_data, train_labels, train_log_stream_max)\n",
    "    val_dataset = EarthquakeDataset(val_data, val_labels, val_log_stream_max)\n",
    "    test_dataset = EarthquakeDataset(test_data, test_labels, test_log_stream_max)\n",
    "\n",
    "    # Create dataloaders\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Load pre-trained best model\n",
    "    best_model_path = \"best_model_seed_1024.pth\"\n",
    "    assert os.path.isfile(best_model_path), f\"Pre-trained model not found at {best_model_path}\"\n",
    "\n",
    "    print(\"\\nLoading pre-trained model...\")\n",
    "    model = EarthquakeModel()\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Perform transfer learning\n",
    "    print(\"\\nStarting transfer learning...\")\n",
    "    transfer_model_path = 'transfer_learning_model.pth'\n",
    "    train_losses, val_losses = transfer_learning(model, train_loader, val_loader,\n",
    "                                               num_epochs=300, patience=50,\n",
    "                                               model_path=transfer_model_path)\n",
    "\n",
    "    # Evaluate transfer-learned model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    mae, mean_pred, true_values, epistemic_unc, aleatoric_unc, combined_unc = \\\n",
    "        evaluate_model(model, test_loader, model_path=transfer_model_path)\n",
    "\n",
    "    # Print results\n",
    "    print(f'\\nTransfer Learning Results:')\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'Mean Aleatoric Uncertainty: {np.mean(aleatoric_unc):.4f}')\n",
    "    print(f'Mean Epistemic Uncertainty: {np.mean(epistemic_unc):.4f}')\n",
    "    print(f'Mean Combined Uncertainty: {np.mean(combined_unc):.4f}')\n",
    "\n",
    "    # Plot uncertainties\n",
    "    print(\"\\nGenerating plots...\")\n",
    "    plot_uncertainties(true_values, mean_pred, epistemic_unc, \n",
    "                      aleatoric_unc, combined_unc, seed)\n",
    "\n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('transfer_learning_loss_curves.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save results\n",
    "    results = {\n",
    "        \"MAE\": float(mae),\n",
    "        \"Mean_Aleatoric_Uncertainty\": float(np.mean(aleatoric_unc)),\n",
    "        \"Mean_Epistemic_Uncertainty\": float(np.mean(epistemic_unc)),\n",
    "        \"Mean_Combined_Uncertainty\": float(np.mean(combined_unc))\n",
    "    }\n",
    "\n",
    "    with open('transfer_learning_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {(end_time - start_time)/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
