# -*- coding: utf-8 -*-
"""Copy of MO_INSTANCE_Dataset_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZtgPi5JUePatSl3Tx45s608mlBEXDKoL
"""

# import os

# # Set the environment variable for SeisBench cache root
# os.environ['SEISBENCH_CACHE_ROOT'] = '/cs/student/projects1/dsml/2023/jingfagu/SCEDC/.seisbench'

# # Verify the environment variable
# print(f"SEISBENCH_CACHE_ROOT is set to: {os.environ.get('SEISBENCH_CACHE_ROOT')}")

# # Create the cache directory if it doesn't exist
# cache_dir = os.environ['SEISBENCH_CACHE_ROOT']
# os.makedirs(cache_dir, exist_ok=True)

# # Now import and use SeisBench
# import seisbench
# import seisbench.data as sbd

# # Verify that SeisBench is using the correct cache root
# print(f"SeisBench cache root: {seisbench.cache_root}")

# # Load a dataset
# scedc = sbd.SCEDC(force=True)
# print(f"Dataset path: {scedc.path}")

# # List the contents of the cache directory
# print(f"Contents of cache directory:")
# print(os.listdir(cache_dir))

# # Check if the SCEDC dataset directory exists
# scedc_dir = os.path.join(cache_dir, 'datasets', 'scedc')
# if os.path.exists(scedc_dir):
#     print(f"SCEDC directory exists at: {scedc_dir}")
#     print(f"Contents of SCEDC directory:")
#     print(os.listdir(scedc_dir))
# else:
#     print(f"SCEDC directory not found at: {scedc_dir}")

# !pip install seisbench
# from google.colab import drive
# import os
# import seisbench.data as sbd

# # Mount Google Drive
# drive.mount('/content/drive')

# # Set the SeisBench cache root to a folder in your Google Drive
# os.environ['SEISBENCH_CACHE_ROOT'] = '/content/drive/MyDrive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/INSTANCE/seisbench_cache/.seisbench'

# # Print the new cache root to verify
# print(f"SeisBench cache root: {os.environ['SEISBENCH_CACHE_ROOT']}")

# # Load the INSTANCE dataset
# instance = sbd.InstanceCounts()

# # Print information about the cached data
# print(f"INSTANCE dataset path: {instance.path}")
# print(f"Files in INSTANCE dataset folder:")
# print(os.listdir(instance.path))

!pip install seisbench
from google.colab import drive
import os
import seisbench.data as sbd

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import torch
from obspy.clients.fdsn import Client
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import os
import ast

# Load and access the metadata from the CSV file
metadata_path = "/content/drive/My Drive/INSTANCE_Dataset/metadata.csv"
metadata = pd.read_csv(metadata_path, low_memory=False)
print(f"Initial number of metadata entries: {len(metadata)}")

import seisbench
import seisbench.data as sbd
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
import os
import time
import scipy.signal
from tqdm import tqdm
from datetime import datetime, timezone
import shutil

# Ensure the 'source_origin_time' column is in datetime format
metadata['source_origin_time'] = pd.to_datetime(metadata['source_origin_time'])

# Sort by source_origin_time to ensure chronological order
metadata = metadata.sort_values(by='source_origin_time')

# Filter metadata for dates from September 2018 onwards
start_date = datetime(2018, 9, 1, tzinfo=timezone.utc)
metadata = metadata[metadata['source_origin_time'] >= start_date]

print(f"Number of metadata entries from 1st September 2018 to 31st January 2020: {len(metadata)}")

# Plot the histogram of the earthquake magnitudes before filtering
plt.figure(figsize=(10, 6))
sns.histplot(metadata["source_magnitude"], bins=30, kde=True)
plt.xlabel('Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.grid(True)
max_magnitude = metadata["source_magnitude"].max()
min_magnitude = metadata["source_magnitude"].min()
plt.text(0.7*max_magnitude, plt.gca().get_ylim()[1]*0.8,
         f'Max: {max_magnitude:.2f} M\nMin: {min_magnitude:.2f} M',
         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)
plt.tight_layout()
plt.show()

print(f"Number of metadata entries now: {len(metadata)}")

metadata['trace_sampling_rate_hz'] = 100

if 'trace_sampling_rate_hz' in metadata.columns:
    print("trace_sampling_rate_hz is present")
else:
    print("trace_sampling_rate_hz is not present")

import copy

# Create a deep copy of the metadata
filtered_metadata = copy.deepcopy(metadata)

# Adjusted filtering for INSTANCE dataset with 30s window
filters = [
    ('path_ep_distance_km <= 110', filtered_metadata.path_ep_distance_km <= 110),
    ('source_magnitude_type == ML', filtered_metadata.source_magnitude_type == 'ML'),
    ('trace_P_arrival_sample.notnull()', filtered_metadata.trace_P_arrival_sample.notnull()),
    ('path_travel_time_P_s.notnull()', filtered_metadata.path_travel_time_P_s.notnull()),
    ('path_travel_time_P_s > 0', filtered_metadata.path_travel_time_P_s > 0),
    ('path_ep_distance_km.notnull()', filtered_metadata.path_ep_distance_km.notnull()),
    ('path_ep_distance_km > 0', filtered_metadata.path_ep_distance_km > 0),
    ('source_depth_km.notnull()', filtered_metadata.source_depth_km.notnull()),
    ('source_magnitude.notnull()', filtered_metadata.source_magnitude.notnull()),
    ('path_backazimuth_deg.notnull()', filtered_metadata.path_backazimuth_deg.notnull()),
    ('path_backazimuth_deg > 0', filtered_metadata.path_backazimuth_deg > 0),
]

# Apply filters one by one and keep track of the data
for desc, filt in filters:
    filtered_metadata = filtered_metadata[filt]
    print(f"After filter '{desc}': {len(filtered_metadata)} entries")

# Print P-arrival statistics before applying time window constraints
print("\nP arrival sample distribution before time window filtering:")
print(filtered_metadata.trace_P_arrival_sample.describe())

# Ensure at least 30 seconds of data after P-arrival
filtered_metadata = filtered_metadata[filtered_metadata.trace_P_arrival_sample + filtered_metadata.trace_sampling_rate_hz * 30 <= filtered_metadata.trace_npts]
print(f"After ensuring 30s after P arrival: {len(filtered_metadata)} entries")

# Calculate time window statistics
filtered_metadata['window_start'] = filtered_metadata.trace_P_arrival_sample - filtered_metadata.trace_sampling_rate_hz * 5  # 5 seconds before P arrival
filtered_metadata['window_end'] = filtered_metadata.trace_P_arrival_sample + filtered_metadata.trace_sampling_rate_hz * 25  # 25 seconds after P arrival (total 30s window)

# Ensure window start is not negative
filtered_metadata = filtered_metadata[filtered_metadata.window_start >= 0]
print(f"After ensuring non-negative window start: {len(filtered_metadata)} entries")

print(f"\nFinal number of filtered metadata entries: {len(filtered_metadata)}")

# Display some statistics about the filtered dataset
print("\nFiltered Dataset Statistics:")
print(f"Magnitude range: {filtered_metadata.source_magnitude.min()} - {filtered_metadata.source_magnitude.max()}")
print(f"Depth range: {filtered_metadata.source_depth_km.min()} - {filtered_metadata.source_depth_km.max()} km")
print(f"Distance range: {filtered_metadata.path_ep_distance_km.min()} - {filtered_metadata.path_ep_distance_km.max()} km")

# Additional statistics
print("\nMagnitude distribution:")
print(filtered_metadata.source_magnitude.describe())

print("\nDepth distribution:")
print(filtered_metadata.source_depth_km.describe())

print("\nP arrival sample distribution:")
print(filtered_metadata.trace_P_arrival_sample.describe())

# Check for S arrivals in the filtered dataset
s_arrivals = filtered_metadata[filtered_metadata.trace_S_arrival_sample.notnull()]
print(f"\nNumber of entries with S arrivals: {len(s_arrivals)}")
if len(s_arrivals) > 0:
    print("\nS arrival sample distribution:")
    print(s_arrivals.trace_S_arrival_sample.describe())

print("\nWindow start sample distribution:")
print(filtered_metadata.window_start.describe())

print("\nWindow end sample distribution:")
print(filtered_metadata.window_end.describe())

# Check if all windows are within the trace
valid_windows = (filtered_metadata.window_start >= 0) & (filtered_metadata.window_end <= filtered_metadata.trace_npts)
print(f"\nNumber of entries with valid 30s windows: {valid_windows.sum()}")

# Verify that the original metadata is unchanged
print(f"\nOriginal number of metadata entries: {len(metadata)}")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import torch
import scipy.signal

# Apply SNR filter
def string_convertor(dd):
    if isinstance(dd, str):
        dd2 = dd.split()
        SNR = []
        for d in dd2:
            if d not in ['[', ']']:
                dL = d.split('[')
                dR = d.split(']')
                if len(dL) == 2:
                    dig = dL[1]
                elif len(dR) == 2:
                    dig = dR[0]
                elif len(dR) == 1 and len(dL) == 1:
                    dig = d
                try:
                    dig = float(dig)
                except Exception:
                    dig = None
                SNR.append(dig)
        return np.mean([x for x in SNR if x is not None])
    else:
        return np.nan

if 'snr_db' in metadata.columns:
    print("snr_db is present")
else:
    print("snr_db is not present")

# Check if 'snr_db' column exists, if not, use appropriate column names from INSTANCE dataset
if 'snr_db' in filtered_metadata.columns:
    filtered_metadata['snr_db'] = filtered_metadata.snr_db.apply(string_convertor)
    filtered_metadata = filtered_metadata[filtered_metadata.snr_db >= 20]
else:
    # Use appropriate SNR columns from INSTANCE dataset
    snr_columns = ['trace_E_snr_db', 'trace_N_snr_db', 'trace_Z_snr_db']
    filtered_metadata['avg_snr_db'] = filtered_metadata[snr_columns].mean(axis=1)
    filtered_metadata = filtered_metadata[filtered_metadata.avg_snr_db >= 20]

print(f"Number of records after SNR filtering: {len(filtered_metadata)}")

# Plot the histogram of the earthquake magnitudes after filtering
plt.figure(figsize=(10, 6))
sns.histplot(filtered_metadata["source_magnitude"], bins=30, kde=True, color='brown')
plt.xlabel('Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.grid(True)
max_magnitude = filtered_metadata["source_magnitude"].max()
min_magnitude = filtered_metadata["source_magnitude"].min()
plt.text(0.7*max_magnitude, plt.gca().get_ylim()[1]*0.8,
         f'Max: {max_magnitude:.2f} M\nMin: {min_magnitude:.2f} M',
         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)
plt.tight_layout()
plt.show()

import h5py

def explore_hdf5_structure(file_path):
    def print_structure(name, obj):
        print(name)
        if isinstance(obj, h5py.Group):
            print(f"  Group: {list(obj.keys())}")
        elif isinstance(obj, h5py.Dataset):
            print(f"  Dataset: shape={obj.shape}, dtype={obj.dtype}")

    with h5py.File(file_path, 'r') as f:
        f.visititems(print_structure)

# Use this function to explore the structure
hdf5_path = "/content/drive/My Drive/INSTANCE_Dataset/waveforms.hdf5"
explore_hdf5_structure(hdf5_path)

def load_data(instance_dataset, metadata):
    X = []
    Y = []
    log_stream_max = []

    hdf5_path = "/content/drive/My Drive/INSTANCE_Dataset/waveforms.hdf5"

    with h5py.File(hdf5_path, 'r') as hdf:
        for _, row in tqdm(metadata.iterrows(), total=len(metadata)):
            try:
                trace_name = row['trace_name']
                sampling_rate = row['trace_sampling_rate_hz']

                # Parse the trace name
                bucket, trace_info = trace_name.split('$')
                trace_index = int(trace_info.split(',')[0])

                # Retrieve waveform data
                waveform = np.array(hdf['data'][bucket][trace_index])

                print(f"Processing {trace_name}")
                print(f"Waveform shape: {waveform.shape}")

                spt = int(row['trace_P_arrival_sample'])
                print(f"P arrival sample: {spt}")

                # Adjust window size based on sampling rate
                window_samples = int(30 * sampling_rate)  # 30 seconds window
                start = max(0, spt - int(5 * sampling_rate))  # 5 seconds before P arrival
                end = start + window_samples

                print(f"Window: start={start}, end={end}, window_samples={window_samples}")

                if start >= waveform.shape[1] or end > waveform.shape[1]:
                    print(f"Skipping event {trace_name}: Invalid window")
                    continue

                dshort = waveform[:, start:end]
                print(f"Windowed waveform shape: {dshort.shape}")

                # Ensure the shape is correct
                if dshort.shape[1] != window_samples:
                    print(f"Skipping event {trace_name}: Incorrect window size")
                    continue

                # Resample to 100 Hz if necessary
                if sampling_rate != 100:
                    dshort = scipy.signal.resample(dshort, (3, 3000), axis=1)

                X.append(dshort)
                Y.append(round(float(row['source_magnitude']), 2))

                # Extract log_stream_max
                max_values = np.max(np.abs(dshort), axis=1)
                log_max_values = np.log10(max_values + 1e-10)
                log_stream_max.append(log_max_values)

            except Exception as e:
                print(f"Error processing event {trace_name}: {e}")
                continue

    X = np.array(X)
    Y = np.array(Y)
    log_stream_max = np.array(log_stream_max)

    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32), torch.tensor(log_stream_max, dtype=torch.float32)

import h5py

# Load and preprocess data
data, labels, log_stream_max = load_data(hdf5_path, filtered_metadata)

print(f"Data shape: {data.shape}")
print(f"Labels shape: {labels.shape}")
print(f"Log stream max shape: {log_stream_max.shape}")

# # Save preprocessed data to Google Drive
# output_data_file = "/content/drive/MyDrive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/INSTANCE/INSTANCE_pre_processed_data.npy"
# output_labels_file = "/content/drive/MyDrive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/INSTANCE/INSTANCE_pre_processed_labels.npy"
# output_log_stream_max_file = "/content/drive/MyDrive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/INSTANCE/INSTANCE_log_stream_max.npy"

# np.save(output_data_file, data.numpy())
# np.save(output_labels_file, labels.numpy())
# np.save(output_log_stream_max_file, log_stream_max.numpy())

# print(f"Preprocessed data saved to {output_data_file}")
# print(f"Preprocessed labels saved to {output_labels_file}")
# print(f"Log stream max values saved to {output_log_stream_max_file}")

print("Job done!!!")



"""# Recalculate max_values"""

# Recalculate max_values
max_values = 10**(np.array(log_stream_max)) - 1e-10

# Ensure the shape matches log_stream_max
assert max_values.shape == log_stream_max.shape, "Shapes do not match!"

# Print the shape to confirm
print("Recovered max_values shape:", max_values.shape)

import time
import json
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
import h5py

from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import time



# Plot histograms of max values and log max values for the vertical component
vertical_index = 0

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(max_values[:, vertical_index], bins=50)  # No need for .numpy() here
plt.xlabel('Stream Max Value', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)

plt.subplot(1, 2, 2)
plt.hist(log_stream_max[:, vertical_index], bins=50)  # No need for .numpy() here
plt.xlabel('Log Stream Max Value', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)

plt.tight_layout()
plt.show()

# Scatter plot of max values vs. labels for the vertical component
plt.figure(figsize=(10, 6))
scatter = plt.scatter(labels.numpy(), max_values[:, vertical_index], alpha=0.5, c=labels.numpy(), cmap='viridis')
plt.xlabel('Earthquake Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Stream Max Amplitude (Vertical)', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
cbar = plt.colorbar(scatter)
cbar.set_label('Density', fontweight='bold', fontsize=14)
cbar.ax.tick_params(labelsize=14)
plt.tight_layout()
plt.show()

# Scatter plot of log max values vs. labels for the vertical component
plt.figure(figsize=(10, 6))
scatter_log = plt.scatter(labels.numpy(), log_stream_max[:, vertical_index], alpha=0.5, c=labels.numpy(), cmap='viridis')
plt.xlabel('Earthquake Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Log Stream Max Amplitude (Vertical)', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
cbar = plt.colorbar(scatter_log)
cbar.set_label('Density', fontweight='bold', fontsize=14)
cbar.ax.tick_params(labelsize=14)
plt.tight_layout()
plt.show()

# Additional analysis: Correlation between vertical max amplitude and magnitude
correlation = np.corrcoef(max_values[:, vertical_index], labels.numpy())[0, 1]
print(f"\nCorrelation between vertical stream max amplitude and earthquake magnitude: {correlation:.4f}")

# Additional analysis: Correlation between vertical log max amplitude and magnitude
correlation_log = np.corrcoef(log_stream_max[:, vertical_index], labels.numpy())[0, 1]
print(f"\nCorrelation between vertical log stream max amplitude and earthquake magnitude: {correlation_log:.4f}")





"""# Optional (Multi-observations >= 400)"""

print(f"Number of records now: {len(filtered_metadata)}")

# Implementing multi-observations with a threshold of >= 400
uniq_ins = filtered_metadata.station_code.unique()
labM = []
for ii in uniq_ins:
    stn = sum(filtered_metadata.station_code == ii)
    if stn >= 400:
        labM.append(ii)

print(f"Number of multi-observations: {len(labM)}")

# Filter the dataset to include only multi-observations
multi_obs_metadata = filtered_metadata[filtered_metadata.station_code.isin(labM)]

print(f"Number of records after multi-observation filtering: {len(multi_obs_metadata)}")

# Get the list of valid events
valid_events = multi_obs_metadata.trace_name.tolist()

def load_data_MO(instance_dataset, metadata):
    X = []
    Y = []
    log_stream_max = []

    hdf5_path = "/content/drive/My Drive/INSTANCE_Dataset/waveforms.hdf5"

    with h5py.File(hdf5_path, 'r') as hdf:
        for _, row in tqdm(metadata.iterrows(), total=len(metadata)):
            try:
                trace_name = row['trace_name']
                sampling_rate = row['trace_sampling_rate_hz']

                # Parse the trace name
                bucket, trace_info = trace_name.split('$')
                trace_index = int(trace_info.split(',')[0])

                # Retrieve waveform data
                waveform = np.array(hdf['data'][bucket][trace_index])

                print(f"Processing {trace_name}")
                print(f"Waveform shape: {waveform.shape}")

                spt = int(row['trace_P_arrival_sample'])
                print(f"P arrival sample: {spt}")

                # Adjust window size based on sampling rate
                window_samples = int(30 * sampling_rate)  # 30 seconds window
                start = max(0, spt - int(5 * sampling_rate))  # 5 seconds before P arrival
                end = start + window_samples

                print(f"Window: start={start}, end={end}, window_samples={window_samples}")

                if start >= waveform.shape[1] or end > waveform.shape[1]:
                    print(f"Skipping event {trace_name}: Invalid window")
                    continue

                dshort = waveform[:, start:end]
                print(f"Windowed waveform shape: {dshort.shape}")

                # Ensure the shape is correct
                if dshort.shape[1] != window_samples:
                    print(f"Skipping event {trace_name}: Incorrect window size")
                    continue

                # Resample to 100 Hz if necessary
                if sampling_rate != 100:
                    dshort = scipy.signal.resample(dshort, (3, 3000), axis=1)

                X.append(dshort)
                Y.append(round(float(row['source_magnitude']), 2))

                # Extract log_stream_max
                max_values = np.max(np.abs(dshort), axis=1)
                log_max_values = np.log10(max_values + 1e-10)
                log_stream_max.append(log_max_values)

            except Exception as e:
                print(f"Error processing event {trace_name}: {e}")
                continue

    X = np.array(X)
    Y = np.array(Y)
    log_stream_max = np.array(log_stream_max)

    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32), torch.tensor(log_stream_max, dtype=torch.float32)

# Load and preprocess data
hdf5_path = "/content/drive/My Drive/INSTANCE_Dataset/waveforms.hdf5"
data_MO, labels_MO, log_stream_max_MO = load_data_MO(hdf5_path, multi_obs_metadata)

print(f"Data shape: {data_MO.shape}")
print(f"Labels shape: {labels_MO.shape}")
print(f"Log stream max shape: {log_stream_max_MO.shape}")

# # Save preprocessed data to Google Drive
# output_data_file_MO = "/content/drive/MyDrive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Notebooks/EDA/INSTANCE/MO_INSTANCE_pre_processed_data.npy"
# output_labels_file_MO = "/content/drive/MyDrive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Notebooks/EDA/INSTANCE/MO_INSTANCE_pre_processed_labels.npy"
# output_log_stream_max_file_MO = "/content/drive/MyDrive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Notebooks/EDA/INSTANCE/MO_INSTANCE_log_stream_max.npy"

# np.save(output_data_file_MO, data.numpy())
# np.save(output_labels_file_MO, labels.numpy())
# np.save(output_log_stream_max_file_MO, log_stream_max.numpy())

# print(f"Preprocessed data saved to {output_data_file_MO}")
# print(f"Preprocessed labels saved to {output_labels_file_MO}")
# print(f"Log stream max values saved to {output_log_stream_max_file_MO}")

print("Multi-observations >= 400: Job done!!!")

# Recalculate max_values
max_values_MO = 10**(np.array(log_stream_max_MO)) - 1e-10

# Ensure the shape matches log_stream_max
assert max_values_MO.shape == log_stream_max_MO.shape, "Shapes do not match!"

# Print the shape to confirm
print("Recovered max_values shape:", max_values_MO.shape)

# Plot histograms of max values and log max values for the vertical component
vertical_index_MO = 0

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(max_values_MO[:, vertical_index_MO], bins=50)  # No need for .numpy() here
plt.xlabel('Stream Max Value', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)

plt.subplot(1, 2, 2)
plt.hist(log_stream_max_MO[:, vertical_index_MO], bins=50)  # No need for .numpy() here
plt.xlabel('Log Stream Max Value', fontweight='bold', fontsize=14)
plt.ylabel('Number', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)

plt.tight_layout()
plt.show()

# Scatter plot of max values vs. labels for the vertical component
plt.figure(figsize=(10, 6))
scatter = plt.scatter(labels_MO.numpy(), max_values_MO[:, vertical_index_MO], alpha=0.5, c=labels_MO.numpy(), cmap='viridis')
plt.xlabel('Earthquake Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Stream Max Amplitude (Vertical)', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
cbar = plt.colorbar(scatter)
cbar.set_label('Density', fontweight='bold', fontsize=14)
cbar.ax.tick_params(labelsize=14)
plt.tight_layout()
plt.show()

# Scatter plot of log max values vs. labels for the vertical component
plt.figure(figsize=(10, 6))
scatter_log = plt.scatter(labels_MO.numpy(), log_stream_max_MO[:, vertical_index_MO], alpha=0.5, c=labels_MO.numpy(), cmap='viridis')
plt.xlabel('Earthquake Magnitude', fontweight='bold', fontsize=14)
plt.ylabel('Log Stream Max Amplitude (Vertical)', fontweight='bold', fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=14)
cbar = plt.colorbar(scatter_log)
cbar.set_label('Density', fontweight='bold', fontsize=14)
cbar.ax.tick_params(labelsize=14)
plt.tight_layout()
plt.show()

# Additional analysis: Correlation between vertical max amplitude and magnitude
correlation_MO = np.corrcoef(max_values_MO[:, vertical_index_MO], labels_MO.numpy())[0, 1]
print(f"\nCorrelation between vertical stream max amplitude and earthquake magnitude: {correlation_MO:.4f}")

# Additional analysis: Correlation between vertical log max amplitude and magnitude
correlation_log_MO = np.corrcoef(log_stream_max_MO[:, vertical_index_MO], labels_MO.numpy())[0, 1]
print(f"\nCorrelation between vertical log stream max amplitude and earthquake magnitude: {correlation_log_MO:.4f}")

# Plot a sample waveform
sample_index = 4500
plt.figure(figsize=(15, 10))
for i in range(3):
    plt.subplot(3, 1, i+1)
    plt.plot(data_MO[sample_index][i])
    plt.title(f'Component {i+1}')
plt.tight_layout()
plt.show()

# Plot a sample waveform
sample_index = 0
plt.figure(figsize=(15, 10))
for i in range(3):
    plt.subplot(3, 1, i+1)
    plt.plot(data_MO[sample_index][i])
    plt.title(f'Component {i+1}')
plt.tight_layout()
plt.show()

print(filtered_metadata.columns.tolist())

import numpy as np
import matplotlib.pyplot as plt

def get_approximate_picks(index):
    # Approximate picks based on visual inspection
    approximate_picks = [
        (7, 10, 28),  # Waveform 1: (P-arrival, S-arrival, Coda end)
        (9, 12, 27),  # Waveform 2
        (6, 7, 15),   # Waveform 3
        (7, 10, 25),  # Waveform 4
        (10, 13, 28)  # Waveform 5
    ]
    return approximate_picks[index]

def plot_instance_waveform(waveform, p_arrival, s_arrival, coda_end, index, sampling_rate=100):
    fig, axs = plt.subplots(3, 1, figsize=(15, 10), sharex=False) # sharex=True to share the x-axis, if set to False, each subplot will have its own x-axis
    time_axis = np.arange(0, waveform.shape[1]) / sampling_rate
    component_names = ['E-W', 'N-S', 'Vertical']

    for i in range(3):
        axs[i].plot(time_axis, waveform[i], 'k')
        axs[i].set_title(f"Waveform {index+1}: {component_names[i]} Component", fontweight='bold', fontsize=14)
        axs[i].set_ylabel('Amplitude counts', fontweight='bold', fontsize=14)
        axs[i].axhline(y=0, color='r', linestyle='--')
        axs[i].set_ylim(np.min(waveform[i])-100, np.max(waveform[i])+100)

        axs[i].axvline(x=p_arrival, color='b', linewidth=2, label='P-arrival')
        axs[i].axvline(x=s_arrival, color='r', linewidth=2, label='S-arrival')
        axs[i].axvline(x=coda_end, color='aqua', linewidth=2, label='Coda End')

        axs[i].legend(loc='upper right', fontsize=14)
        axs[i].tick_params(axis='both', which='major', labelsize=14)

    axs[2].set_xlabel('Time (s)', fontweight='bold', fontsize=14)
    plt.tight_layout()
    plt.show()

# Plot multiple sample waveforms
sample_indices = [0, 1, 2, 3, 4]

for idx in sample_indices:
    waveform = data_MO[idx].numpy()
    p_arrival, s_arrival, coda_end = get_approximate_picks(idx)

    plot_instance_waveform(waveform, p_arrival, s_arrival, coda_end, idx)

