{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: Initial Setup with SeisBench Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Seismogram_Based_MO_INSTANCE_Preprocessing.ipynb\n",
    "\n",
    "This notebook processes the INSTANCE dataset for seismogram-based splitting, \n",
    "which treats each seismogram as an independent sample.\n",
    "\"\"\"\n",
    "\n",
    "# Install and import required packages\n",
    "!pip install seisbench\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install h5py pandas tqdm matplotlib torchinfo seaborn\n",
    "\n",
    "from google.colab import drive\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.signal\n",
    "import seisbench.data as sbd\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Mount Google Drive for output directory only\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set SeisBench to use local VM storage to avoid quota issues\n",
    "os.environ['SEISBENCH_CACHE_ROOT'] = '/content/.seisbench'\n",
    "print(f\"SeisBench cache root: {os.environ['SEISBENCH_CACHE_ROOT']}\")\n",
    "\n",
    "# Download INSTANCE dataset using SeisBench\n",
    "print(\"Downloading INSTANCE dataset using SeisBench...\")\n",
    "print(\"This will download to local VM storage, avoiding Google Drive quota issues...\")\n",
    "instance = sbd.InstanceCounts()\n",
    "\n",
    "# Get paths to the downloaded files\n",
    "instance_path = instance.path\n",
    "metadata_path = os.path.join(instance_path, \"metadata.csv\")\n",
    "hdf5_path = os.path.join(instance_path, \"waveforms.hdf5\")\n",
    "\n",
    "# Output directory for seismogram-based results\n",
    "output_dir = \"/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/INSTANCE_Seismogram_Based\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"INSTANCE dataset path: {instance_path}\")\n",
    "print(f\"Files in INSTANCE dataset folder:\")\n",
    "print(os.listdir(instance_path))\n",
    "\n",
    "assert os.path.isfile(metadata_path), f\"Metadata file not found at {metadata_path}\"\n",
    "assert os.path.isfile(hdf5_path), f\"HDF5 file not found at {hdf5_path}\"\n",
    "\n",
    "print(f\"✓ Using SeisBench INSTANCE files:\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(f\"  HDF5: {hdf5_path}\")\n",
    "print(f\"✓ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Data Loading and Initial Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 2: Data Loading and Initial Filtering\"\"\"\n",
    "\n",
    "# Load and access the metadata from the CSV file\n",
    "metadata = pd.read_csv(metadata_path, low_memory=False)\n",
    "print(f\"Initial number of metadata entries: {len(metadata)}\")\n",
    "\n",
    "# Ensure the 'source_origin_time' column is in datetime format\n",
    "metadata['source_origin_time'] = pd.to_datetime(metadata['source_origin_time'])\n",
    "\n",
    "# Sort by source_origin_time to ensure chronological order\n",
    "metadata = metadata.sort_values(by='source_origin_time')\n",
    "\n",
    "# # Filter metadata for dates from September 2018 onwards\n",
    "# from datetime import datetime, timezone\n",
    "# start_date = datetime(2018, 9, 1, tzinfo=timezone.utc)\n",
    "# metadata = metadata[metadata['source_origin_time'] >= start_date]\n",
    "# print(f\"Number of metadata entries from 1st September 2018 to 31st January 2020: {len(metadata)}\")\n",
    "\n",
    "# Plot the histogram of the earthquake magnitudes before filtering\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(metadata[\"source_magnitude\"], bins=30, kde=True)\n",
    "plt.xlabel('Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "max_magnitude = metadata[\"source_magnitude\"].max()\n",
    "min_magnitude = metadata[\"source_magnitude\"].min()\n",
    "plt.text(0.7*max_magnitude, plt.gca().get_ylim()[1]*0.8,\n",
    "         f'Max: {max_magnitude:.2f} M\\nMin: {min_magnitude:.2f} M',\n",
    "         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)\n",
    "plt.title('INSTANCE Dataset - Magnitude Distribution (Before Filtering)', fontweight='bold', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of metadata entries now: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Apply Data Filtering Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 3: Apply Data Filtering Criteria\"\"\"\n",
    "\n",
    "import copy\n",
    "\n",
    "# Create a deep copy of the metadata\n",
    "filtered_metadata = copy.deepcopy(metadata)\n",
    "\n",
    "# Set sampling rate\n",
    "filtered_metadata['trace_sampling_rate_hz'] = 100\n",
    "\n",
    "# Adjusted filtering for INSTANCE dataset with 30s window\n",
    "filters = [\n",
    "    ('path_ep_distance_km <= 110', filtered_metadata.path_ep_distance_km <= 110),\n",
    "    ('source_magnitude_type == ML', filtered_metadata.source_magnitude_type == 'ML'),\n",
    "    ('trace_P_arrival_sample.notnull()', filtered_metadata.trace_P_arrival_sample.notnull()),\n",
    "    ('path_travel_time_P_s.notnull()', filtered_metadata.path_travel_time_P_s.notnull()),\n",
    "    ('path_travel_time_P_s > 0', filtered_metadata.path_travel_time_P_s > 0),\n",
    "    ('path_ep_distance_km.notnull()', filtered_metadata.path_ep_distance_km.notnull()),\n",
    "    ('path_ep_distance_km > 0', filtered_metadata.path_ep_distance_km > 0),\n",
    "    ('source_depth_km.notnull()', filtered_metadata.source_depth_km.notnull()),\n",
    "    ('source_magnitude.notnull()', filtered_metadata.source_magnitude.notnull()),\n",
    "    ('path_backazimuth_deg.notnull()', filtered_metadata.path_backazimuth_deg.notnull()),\n",
    "    ('path_backazimuth_deg > 0', filtered_metadata.path_backazimuth_deg > 0),\n",
    "]\n",
    "\n",
    "# Apply filters one by one and keep track of the data\n",
    "for desc, filt in filters:\n",
    "    filtered_metadata = filtered_metadata[filt]\n",
    "    print(f\"After filter '{desc}': {len(filtered_metadata)} entries\")\n",
    "\n",
    "# Ensure at least 30 seconds of data after P-arrival\n",
    "filtered_metadata = filtered_metadata[filtered_metadata.trace_P_arrival_sample + filtered_metadata.trace_sampling_rate_hz * 30 <= filtered_metadata.trace_npts]\n",
    "print(f\"After ensuring 30s after P arrival: {len(filtered_metadata)} entries\")\n",
    "\n",
    "# Calculate time window statistics\n",
    "filtered_metadata['window_start'] = filtered_metadata.trace_P_arrival_sample - filtered_metadata.trace_sampling_rate_hz * 5  # 5 seconds before P arrival\n",
    "filtered_metadata['window_end'] = filtered_metadata.trace_P_arrival_sample + filtered_metadata.trace_sampling_rate_hz * 25  # 25 seconds after P arrival (total 30s window)\n",
    "\n",
    "# Ensure window start is not negative\n",
    "filtered_metadata = filtered_metadata[filtered_metadata.window_start >= 0]\n",
    "print(f\"After ensuring non-negative window start: {len(filtered_metadata)} entries\")\n",
    "\n",
    "print(f\"\\nFinal number of filtered metadata entries: {len(filtered_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Apply SNR Filter and Multi-Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 4: Apply SNR Filter and Multi-Observations\"\"\"\n",
    "\n",
    "# Process SNR - same as event-based\n",
    "if 'snr_db' in filtered_metadata.columns:\n",
    "    def string_convertor(dd):\n",
    "        if isinstance(dd, str):\n",
    "            dd2 = dd.split()\n",
    "            SNR = []\n",
    "            for d in dd2:\n",
    "                if d not in ['[', ']']:\n",
    "                    dL = d.split('[')\n",
    "                    dR = d.split(']')\n",
    "                    if len(dL) == 2:\n",
    "                        dig = dL[1]\n",
    "                    elif len(dR) == 2:\n",
    "                        dig = dR[0]\n",
    "                    elif len(dR) == 1 and len(dL) == 1:\n",
    "                        dig = d\n",
    "                    try:\n",
    "                        dig = float(dig)\n",
    "                    except Exception:\n",
    "                        dig = None\n",
    "                    SNR.append(dig)\n",
    "            return np.mean([x for x in SNR if x is not None])\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    filtered_metadata['snr_db'] = filtered_metadata.snr_db.apply(string_convertor)\n",
    "    filtered_metadata = filtered_metadata[filtered_metadata.snr_db >= 20]\n",
    "else:\n",
    "    # Use appropriate SNR columns from INSTANCE dataset\n",
    "    snr_columns = ['trace_E_snr_db', 'trace_N_snr_db', 'trace_Z_snr_db']\n",
    "    filtered_metadata['avg_snr_db'] = filtered_metadata[snr_columns].mean(axis=1)\n",
    "    filtered_metadata = filtered_metadata[filtered_metadata.avg_snr_db >= 20]\n",
    "\n",
    "print(f\"Number of records after SNR filtering: {len(filtered_metadata)}\")\n",
    "\n",
    "# Plot the histogram of the earthquake magnitudes after filtering\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_metadata[\"source_magnitude\"], bins=30, kde=True, color='brown')\n",
    "plt.xlabel('Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number', fontweight='bold', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "max_magnitude = filtered_metadata[\"source_magnitude\"].max()\n",
    "min_magnitude = filtered_metadata[\"source_magnitude\"].min()\n",
    "plt.text(0.7*max_magnitude, plt.gca().get_ylim()[1]*0.8,\n",
    "         f'Max: {max_magnitude:.2f} M\\nMin: {min_magnitude:.2f} M',\n",
    "         bbox=dict(facecolor='none', edgecolor='red', boxstyle='round,pad=1'), fontsize=14)\n",
    "plt.title('INSTANCE Dataset - Magnitude Distribution (After Filtering)', fontweight='bold', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Implementing multi-observations with a threshold of >= 400\n",
    "print(\"Filtering for multi-observations stations...\")\n",
    "uniq_ins = filtered_metadata.station_code.unique()\n",
    "labM = []\n",
    "\n",
    "print(\"Counting observations per station...\")\n",
    "for station in tqdm(uniq_ins):\n",
    "    count = sum(filtered_metadata.station_code == station)\n",
    "    if count >= 400:\n",
    "        labM.append(station)\n",
    "        print(f\"{station}: {count} observations\")\n",
    "\n",
    "print(f\"Number of stations with ≥400 observations: {len(labM)}\")\n",
    "\n",
    "# Save the multi-observations stations list\n",
    "multi_observations_path = os.path.join(output_dir, \"multi_observations.npy\")\n",
    "np.save(multi_observations_path, labM)\n",
    "\n",
    "# Filter the dataset to include only records from stations with ≥400 observations\n",
    "df_filtered = filtered_metadata[filtered_metadata.station_code.isin(labM)]\n",
    "print(f\"Number of records after multi-observations filtering: {len(df_filtered)}\")\n",
    "\n",
    "# For seismogram-based splitting, we note event information but don't group by it\n",
    "print(f\"\\nSeismogram-based approach: treating each of {len(df_filtered)} seismograms independently\")\n",
    "print(f\"Note: This approach allows seismograms from the same event to be in different splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 5: Data Loading Function (Same as Event-Based)\"\"\"\n",
    "\n",
    "# Function to load seismogram data from HDF5 file (same as event-based)\n",
    "def load_seismogram_instance(hdf5_path, seismogram_id, metadata_row):\n",
    "    \"\"\"\n",
    "    Load a single seismogram from the INSTANCE HDF5 file\n",
    "\n",
    "    Args:\n",
    "        hdf5_path: Path to the HDF5 file\n",
    "        seismogram_id: ID of the seismogram to load (trace_name)\n",
    "        metadata_row: Row from metadata containing trace information\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (data, magnitude) or (None, None) if seismogram not found\n",
    "\n",
    "    Notes:\n",
    "        - Data is a 30-second window: 5s before P-arrival and 25s after\n",
    "        - Resampled to 100Hz if necessary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(hdf5_path, 'r') as hdf:\n",
    "            # Parse the trace name to get bucket and trace index\n",
    "            bucket, trace_info = seismogram_id.split('$')\n",
    "            trace_index = int(trace_info.split(',')[0])\n",
    "\n",
    "            # Retrieve waveform data\n",
    "            waveform = np.array(hdf['data'][bucket][trace_index])\n",
    "\n",
    "            sampling_rate = metadata_row['trace_sampling_rate_hz']\n",
    "            spt = int(metadata_row['trace_P_arrival_sample'])\n",
    "\n",
    "            # Adjust window size based on sampling rate\n",
    "            window_samples = int(30 * sampling_rate)  # 30 seconds window\n",
    "            start = max(0, spt - int(5 * sampling_rate))  # 5 seconds before P arrival\n",
    "            end = start + window_samples\n",
    "\n",
    "            if start >= waveform.shape[1] or end > waveform.shape[1]:\n",
    "                print(f\"Skipping event {seismogram_id}: Invalid window\")\n",
    "                return None, None\n",
    "\n",
    "            dshort = waveform[:, start:end]\n",
    "\n",
    "            # Ensure the shape is correct\n",
    "            if dshort.shape[1] != window_samples:\n",
    "                print(f\"Skipping event {seismogram_id}: Incorrect window size\")\n",
    "                return None, None\n",
    "\n",
    "            # Resample to 100 Hz if necessary\n",
    "            if sampling_rate != 100:\n",
    "                dshort = scipy.signal.resample(dshort, 3000, axis=1)\n",
    "\n",
    "            mag = round(float(metadata_row['source_magnitude']), 2)\n",
    "\n",
    "            return dshort, mag\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event {seismogram_id}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: Load All Seismograms (Simplified - No Event Grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 6: Load All Seismograms (Simplified for Seismogram-Based)\"\"\"\n",
    "\n",
    "# Load all valid seismograms - simpler than event-based since no grouping needed\n",
    "print(\"Loading all valid seismograms for seismogram-based splitting...\")\n",
    "all_data = []\n",
    "all_labels = []\n",
    "all_seismogram_ids = []\n",
    "\n",
    "# Create a lookup dictionary for metadata rows\n",
    "metadata_lookup = {row['trace_name']: row for _, row in df_filtered.iterrows()}\n",
    "\n",
    "# Get list of all valid seismogram IDs\n",
    "valid_seismogram_ids = df_filtered['trace_name'].tolist()\n",
    "print(f\"Number of seismograms to load: {len(valid_seismogram_ids)}\")\n",
    "\n",
    "# Load seismograms with progress tracking\n",
    "for seis_id in tqdm(valid_seismogram_ids, desc=\"Loading seismograms\"):\n",
    "    if seis_id not in metadata_lookup:\n",
    "        continue\n",
    "\n",
    "    metadata_row = metadata_lookup[seis_id]\n",
    "    data, label = load_seismogram_instance(hdf5_path, seis_id, metadata_row)\n",
    "\n",
    "    if data is None:\n",
    "        continue\n",
    "\n",
    "    all_data.append(data)\n",
    "    all_labels.append(label)\n",
    "    all_seismogram_ids.append(seis_id)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_data = np.array(all_data)\n",
    "all_labels = np.array(all_labels)\n",
    "all_seismogram_ids = np.array(all_seismogram_ids)\n",
    "\n",
    "print(f\"Final dataset: {len(all_data)} seismograms\")\n",
    "print(f\"Data shape: {all_data.shape}, Labels shape: {all_labels.shape}\")\n",
    "\n",
    "# For comparison with event-based approach\n",
    "unique_events = df_filtered['source_id'].nunique() if 'source_id' in df_filtered.columns else df_filtered['source_event_id'].nunique()\n",
    "avg_seismograms_per_event = len(all_data) / unique_events\n",
    "print(f\"Average seismograms per event: {avg_seismograms_per_event:.2f}\")\n",
    "print(f\"Note: In seismogram-based splitting, these {len(all_data)} seismograms will be split randomly\")\n",
    "print(f\"      regardless of which event they belong to, potentially causing data leakage.\")\n",
    "\n",
    "# Save seismogram IDs mapping\n",
    "with open(os.path.join(output_dir, 'seismogram_indices.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'seismogram_ids': all_seismogram_ids,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: Visualize Sample Seismograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 7: Visualize Sample Seismograms\"\"\"\n",
    "\n",
    "# Plot an example seismogram\n",
    "def plot_example_seismogram_instance(data, labels, index=0):\n",
    "    \"\"\"Plot an example three-component seismogram\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "    time = np.arange(data.shape[2]) / 100  # Convert to seconds (100Hz sampling)\n",
    "    components = ['East-West', 'North-South', 'Vertical']\n",
    "\n",
    "    for i in range(3):\n",
    "        axes[i].plot(time, data[index, i, :], 'k')\n",
    "        axes[i].set_ylabel(f'{components[i]}\\nAmplitude', fontweight='bold', fontsize=14)\n",
    "        axes[i].axvline(x=5.0, color='blue', linestyle='--', linewidth=2,\n",
    "                      label='P-arrival (5s mark)')\n",
    "        axes[i].grid(True)\n",
    "        axes[i].legend(fontsize=12)\n",
    "\n",
    "    axes[2].set_xlabel('Time (seconds)', fontweight='bold', fontsize=14)\n",
    "    plt.suptitle(f'Example INSTANCE Seismogram (Magnitude: {labels[index]})\\nSeismogram-Based Approach',\n",
    "                fontweight='bold', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot multiple example seismograms\n",
    "if len(all_data) > 0:\n",
    "    print(\"Plotting example seismograms for seismogram-based approach:\")\n",
    "    for i in [0, 100, 500]:  # Plot a few different examples\n",
    "        if i < len(all_data):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            plot_example_seismogram_instance(all_data, all_labels, i)\n",
    "\n",
    "# Plot magnitude distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(all_labels, bins=30, kde=True, color='green')\n",
    "plt.xlabel('Magnitude', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Number of Seismograms', fontweight='bold', fontsize=14)\n",
    "plt.title('INSTANCE Seismogram-Based Dataset - Final Magnitude Distribution', fontweight='bold', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: Save Data and Create Split Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 8: Save Data and Create Split Information\"\"\"\n",
    "\n",
    "# Convert to tensors\n",
    "all_data_tensor = torch.tensor(all_data, dtype=torch.float32)\n",
    "all_labels_tensor = torch.tensor(all_labels, dtype=torch.float32)\n",
    "\n",
    "# Save the entire dataset\n",
    "print(\"Saving data files...\")\n",
    "torch.save(all_data_tensor, os.path.join(output_dir, 'all_data.pt'))\n",
    "torch.save(all_labels_tensor, os.path.join(output_dir, 'all_labels.pt'))\n",
    "\n",
    "# Create seismogram-based split information (much simpler than event-based)\n",
    "def generate_seismogram_based_split_info(num_samples, train_ratio=0.7, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Generate split information for seismogram-based random splits\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Total number of seismograms\n",
    "        train_ratio: Proportion for training set\n",
    "        val_ratio: Proportion for validation set\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with split information for random seismogram-based splits\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'num_samples': num_samples,\n",
    "        'train_ratio': train_ratio,\n",
    "        'val_ratio': val_ratio,\n",
    "        'splitting_approach': 'seismogram_based',\n",
    "        'note': 'Seismograms split randomly regardless of event membership'\n",
    "    }\n",
    "\n",
    "# Generate split information\n",
    "print(\"Preparing split information for seismogram-based experiments...\")\n",
    "split_info = generate_seismogram_based_split_info(len(all_data))\n",
    "\n",
    "# Save split information\n",
    "with open(os.path.join(output_dir, 'seismogram_split_info.pkl'), 'wb') as f:\n",
    "    pickle.dump(split_info, f)\n",
    "\n",
    "# Save additional metadata for comparison purposes\n",
    "comparison_data = {\n",
    "    'total_seismograms': len(all_data),\n",
    "    'unique_events': unique_events,\n",
    "    'avg_seismograms_per_event': avg_seismograms_per_event,\n",
    "    'approach': 'seismogram_based',\n",
    "    'potential_data_leakage': True,\n",
    "    'comparison_note': 'Unlike event-based splitting, this approach may have seismograms from the same event in different splits'\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'comparison_info.pkl'), 'wb') as f:\n",
    "    pickle.dump(comparison_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: Summary and Final Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 9: Summary and Final Statistics\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSTANCE SEISMOGRAM-BASED PREPROCESSING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFiles created in the output directory:\")\n",
    "print(f\"1. all_data.pt - All seismogram data tensors\")\n",
    "print(f\"2. all_labels.pt - All magnitude labels\")\n",
    "print(f\"3. seismogram_split_info.pkl - Information for random seismogram-based splits\")\n",
    "print(f\"4. seismogram_indices.pkl - Mapping of seismogram IDs\")\n",
    "print(f\"5. multi_observations.npy - List of stations with ≥400 observations\")\n",
    "print(f\"6. comparison_info.pkl - Comparison data for analysis\")\n",
    "\n",
    "print(f\"\\nFinal Dataset Summary:\")\n",
    "print(f\"Total seismograms: {len(all_data)}\")\n",
    "print(f\"Unique events represented: {unique_events}\")\n",
    "print(f\"Average seismograms per event: {avg_seismograms_per_event:.2f}\")\n",
    "print(f\"Data shape: {all_data.shape}\")\n",
    "\n",
    "print(f\"\\nSplitting Approach: SEISMOGRAM-BASED\")\n",
    "print(f\"- Seismograms will be split randomly regardless of event membership\")\n",
    "print(f\"- This may result in seismograms from the same event appearing in different splits\")\n",
    "print(f\"- Expected to show data leakage effects compared to event-based splitting\")\n",
    "\n",
    "# Report execution time\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "print(f\"\\nINSTANCE seismogram-based preprocessing completed successfully!\")\n",
    "print(f\"Ready for comparison with event-based approach:\")\n",
    "print(f\"   Event-based (10.64 seismograms/event) vs Seismogram-based (random splitting)\")\n",
    "print(f\"Used SeisBench download - no Google Drive quota issues!\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Run INSTANCE_50_Experiments_Seismogram_Based_Splits_Runs_1_to_25.ipynb\")\n",
    "print(f\"2. Run INSTANCE_50_Experiments_Seismogram_Based_Splits_Runs_26_to_50.ipynb\")\n",
    "print(f\"3. Compare results with event-based splitting experiments\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
